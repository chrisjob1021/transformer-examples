{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text Once upon a time\n",
      "input_ids tensor([[7454, 2402,  257,  640]], device='cuda:0')\n",
      "Greedy next token: ,\n",
      "Temperature sampling next token:  incap\n",
      "Top-k sampling next token:  before\n",
      "Once upon a time before\n"
     ]
    }
   ],
   "source": [
    "from models.roformer import RoFormerForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Example text generation\n",
    "input_text = \"Once upon a time\"\n",
    "print(\"input_text\", input_text)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "print(\"input_ids\", input_ids)\n",
    "\n",
    "# Load the trained model\n",
    "model = RoFormerForCausalLM.from_pretrained(\"/home/ubuntu/roformer/roformer-rope-disabled\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate text and apply softmax to logits\n",
    "output = model(input_ids)\n",
    "logits = output['logits']\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the last token's probabilities\n",
    "last_token_probs = probs[:, -1, :]  # shape: [batch_size, vocab_size]\n",
    "\n",
    "# Sample from the probabilities\n",
    "# Option 1: Greedy (just take highest probability)\n",
    "next_token_greedy = torch.argmax(last_token_probs, dim=-1)  # shape: [batch_size]\n",
    "print(\"Greedy next token:\", tokenizer.decode(next_token_greedy))\n",
    "\n",
    "# Option 2: Temperature sampling (more diverse)\n",
    "temperature = 0.7  # lower = more focused, higher = more random\n",
    "scaled_probs = last_token_probs / temperature\n",
    "next_token_temp = torch.multinomial(torch.softmax(scaled_probs, dim=-1), num_samples=1)  # shape: [batch_size, 1]\n",
    "print(\"Temperature sampling next token:\", tokenizer.decode(next_token_temp.squeeze()))\n",
    "\n",
    "# Option 3: Top-k sampling (more controlled)\n",
    "top_k = 50\n",
    "top_k_probs, top_k_indices = torch.topk(last_token_probs, k=top_k)\n",
    "top_k_probs = top_k_probs / temperature\n",
    "top_k_probs = torch.softmax(top_k_probs, dim=-1)\n",
    "next_token_topk = torch.multinomial(top_k_probs, num_samples=1)  # shape: [batch_size, 1]\n",
    "next_token_topk = torch.gather(top_k_indices, 1, next_token_topk)  # shape: [batch_size, 1]\n",
    "print(\"Top-k sampling next token:\", tokenizer.decode(next_token_topk.squeeze()))\n",
    "\n",
    "# Append the new token to your sequence\n",
    "input_ids = torch.cat([input_ids, next_token_topk], dim=1)\n",
    "print(tokenizer.decode(input_ids.squeeze()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
