{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugpy is listening on localhost:5678\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    import debugpy\n",
    "    \n",
    "    # Try to listen on the port, catch exception if already listening\n",
    "    try:\n",
    "        debugpy.listen((\"localhost\", 5678))\n",
    "        print(\"Debugpy is listening on localhost:5678\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Debugpy is already listening: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from utils import TrainingConfig, Config\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # or your custom one\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "training_config = TrainingConfig()\n",
    "config = Config(vocab_size=tokenizer.vocab_size,\n",
    "d_model=768, num_heads=12, ffn_dim=3072,\n",
    "num_layers=12, max_seq_len=tokenizer.model_max_length, enable_rope=True )\n",
    "\n",
    "savepath = \"/home/chrisobrien/roformer-base\"\n",
    "\n",
    "if False:\n",
    "    # Create a config dictionary for the model\n",
    "    config_dict = {k: getattr(config, k) for k in vars(config) \n",
    "                if not k.startswith('_') and not callable(getattr(config, k))}\n",
    "\n",
    "    # Save the config as JSON\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    with open(os.path.join(savepath, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataset_name = \"gpt2_tokenized_concatenated_openwebtext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # 1. Load the raw text\n",
    "    ds = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "    if False:\n",
    "        ds = ds.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    tokenized = load_from_disk(\"gpt2_tokenized_openwebtext\")\n",
    "    tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = tokenized\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=False,\n",
    "            # max_length=config.max_seq_len,\n",
    "            padding=False,\n",
    "            # return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "    tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"], num_proc=24)\n",
    "    # tokenized = tokenized.remove_columns([\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def group_texts(batch):\n",
    "        concatenated = sum(batch[\"input_ids\"], [])\n",
    "        total_length = len(concatenated)\n",
    "        total_length = (total_length // config.max_seq_len) * config.max_seq_len\n",
    "        result = {\n",
    "            \"input_ids\": [concatenated[i:i+config.max_seq_len] for i in range(0, total_length, config.max_seq_len)]\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    lm_dataset = tokenized.remove_columns([\"attention_mask\"])\n",
    "    if False: lm_dataset = lm_dataset.select(range(1000))\n",
    "    lm_dataset = lm_dataset.map(group_texts, batched=True, num_proc=24)\n",
    "    lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Save the tokenized dataset to disk\n",
    "    lm_dataset.save_to_disk(dataset_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c25fb01d494adc9394cdf9aa7184e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    lm_dataset = load_from_disk(dataset_name)\n",
    "    lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 8728153\n",
      "Evaluation dataset size: 88164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 8728153\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 88164\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False: lm_dataset = lm_dataset.select(range(1000))\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_test_split = lm_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Display the datasets\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = lm_dataset\n",
    "    # Let's check a few examples for padding tokens and EOS tokens\n",
    "    for i in range(100):  # Check first 5 examples\n",
    "        # Convert to tensor first, then do the comparison\n",
    "        input_ids = torch.tensor(ds[i][\"input_ids\"])\n",
    "        pad_mask = (input_ids == tokenizer.pad_token_id)\n",
    "        pad_count = pad_mask.sum().item()\n",
    "        \n",
    "        # Check for EOS tokens\n",
    "        eos_mask = (input_ids == tokenizer.eos_token_id)\n",
    "        eos_count = eos_mask.sum().item()\n",
    "        \n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Sequence length: {len(input_ids)}\")\n",
    "        \n",
    "        # Check padding tokens\n",
    "        if pad_count > 0:\n",
    "            print(f\"Pad tokens found: {pad_count}\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"No padding tokens found\")\n",
    "        \n",
    "        # # Check EOS tokens\n",
    "        # if eos_count > 0:\n",
    "        #     print(f\"EOS tokens found: {eos_count}\")\n",
    "        #     # Show where the EOS tokens are\n",
    "        #     eos_positions = (input_ids == tokenizer.eos_token_id).nonzero().flatten().tolist()\n",
    "        #     print(f\"EOS token positions: {eos_positions}\")\n",
    "        # else:\n",
    "        #     print(\"No EOS tokens found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load the tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the tokenized dataset from disk\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "    try:\n",
    "        print(\"Loading tokenized dataset from disk...\")\n",
    "        tokenized = load_from_disk(dataset_name)\n",
    "        print(f\"Successfully loaded tokenized dataset with {len(tokenized)} examples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dataset not found at {dataset_name}. Please make sure you've saved the tokenized dataset first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Save the tokenized dataset to disk\n",
    "    dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "    tokenized.save_to_disk(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Upload to Hugging Face Hub\n",
    "    # You'll need to be logged in to Hugging Face\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # Login to Hugging Face (you'll need to run this once and enter your token)\n",
    "    # Uncomment the line below when you're ready to login\n",
    "    login(\"hf_xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from models import RoFormerForCausalLM, RoFormerDecoder\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "model_base = RoFormerDecoder(config)\n",
    "model = RoFormerForCausalLM(model_base, config)\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Get the absolute path for logs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = os.path.join(os.path.dirname(savepath), \"logs\", f\"run_{timestamp}\")\n",
    "# Create the logging directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=savepath,\n",
    "\n",
    "    learning_rate=6e-4,\n",
    "    # lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,  # 10% of total training steps for warmup\n",
    "    # warmup_steps=2_000,\n",
    "    # Specify AdamW optimizer\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.5,\n",
    "\n",
    "    max_steps=2_000,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=16, # Accumulate gradients over N steps\n",
    "    #With gradient accumulation (gradient_accumulation_steps=8):\n",
    "        # You split what would have been one batch into 8 smaller micro-batches\n",
    "        # For each micro-batch, you:\n",
    "        # Load 1/8th of the data into memory\n",
    "        # Do a forward pass (storing 1/8th of the activations)\n",
    "        # Do a backward pass (computing 1/8th of the gradients)\n",
    "        # ACCUMULATE the gradients (don't update weights yet)\n",
    "        # Clear the activations (but keep gradients)\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_accumulation_steps=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=50,\n",
    "\n",
    "    save_steps=100,\n",
    "    save_total_limit=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_safetensors=False,\n",
    "\n",
    "    gradient_checkpointing=False,\n",
    "    # Must be supported by the model\n",
    "    #With Gradient Checkpointing:\n",
    "        # During the forward pass, only store activations at certain \"checkpoints\"\n",
    "        # During backpropagation, RECOMPUTE the intermediate activations as needed\n",
    "        # This means doing some forward computations twice, but using much less memory\n",
    "    # Without checkpointing, you need to store activations for all 12 layers. With checkpointing, you might only store activations every few layers and recompute the rest during backprop.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/2000 03:02 < 2:00:53, 0.27 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if False:\n",
    "    trainer.train(resume_from_checkpoint=savepath + \"/checkpoint-210\")\n",
    "if True:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4602,   284,   307,  ...,    11,   447,   251],\n",
       "        [   25,  3363,    11,  ...,   355,   340, 14051],\n",
       "        [  407,  2245,  6095,  ...,   198,  2394,   475],\n",
       "        [  262,   640,   262,  ...,   532,   290,  6825]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 4602,   284,   307,  ...,    11,   447,   251],\n",
       "        [   25,  3363,    11,  ...,   355,   340, 14051],\n",
       "        [  407,  2245,  6095,  ...,   198,  2394,   475],\n",
       "        [  262,   640,   262,  ...,   532,   290,  6825]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor([436.6937], device='cuda:0', grad_fn=<UnsqueezeBackward0>),\n",
       " 'logits': tensor([[[ 34.2782,   3.1532,  26.2072,  ...,  24.9944,  29.4306,  -6.2552],\n",
       "          [ 10.0444,  20.4900,  38.4594,  ...,  15.2377,  44.5526, -25.0688],\n",
       "          [  2.3788,  12.9716,  11.8669,  ...,  18.8346,  49.7358, -42.2385],\n",
       "          ...,\n",
       "          [-25.4352,   0.2880,  -3.5974,  ..., -24.5217,  13.2999,  -3.8325],\n",
       "          [-15.6319, -16.8085,  -8.0010,  ...,   9.2895,   0.0699, -58.9580],\n",
       "          [-19.8348,  42.4760, -35.9720,  ...,   2.0393,  22.5927, -20.3634]],\n",
       " \n",
       "         [[  9.3869, -21.8179,  -5.9259,  ...,  18.8216,   1.7853,  -6.8613],\n",
       "          [  3.3824, -14.6326,   2.3107,  ...,  13.4163,  -2.5109,  10.4513],\n",
       "          [ -3.8203,  -4.2022,  -5.0340,  ..., -17.9625,  -9.3158,   0.6999],\n",
       "          ...,\n",
       "          [ 14.9311,  -8.3173, -24.1865,  ...,  43.7960, -42.2001, -38.4019],\n",
       "          [-17.2776,  39.2271,  13.2699,  ..., -20.1246,  29.3764,  -0.3817],\n",
       "          [ 14.7649,  31.5318,   0.0843,  ...,  26.3986, -13.4164,  53.2475]],\n",
       " \n",
       "         [[ 12.3524, -18.5938,  26.3734,  ..., -22.3893, -26.0338, -11.6842],\n",
       "          [-14.9825,  43.0106,  23.4415,  ..., -29.3404,   9.1161, -17.6670],\n",
       "          [  1.3535,  -5.6878, -15.6847,  ..., -11.6638, -14.9981, -34.2915],\n",
       "          ...,\n",
       "          [  4.7214, -44.7105, -33.2032,  ..., -22.4220,  42.0109, -30.4344],\n",
       "          [-30.7935, -12.9163,  -9.7687,  ...,  31.1298,  34.6013, -26.6926],\n",
       "          [-34.6148, -26.2702,   7.6424,  ...,  -9.6046,  52.6844, -23.3199]],\n",
       " \n",
       "         [[-65.3149, -13.2262, -25.6191,  ...,  41.7521,  20.3270,  12.1194],\n",
       "          [-56.7105, -13.0983, -20.3223,  ...,  -4.7009,  15.1532,  22.4792],\n",
       "          [-65.2390,  -4.2278, -15.9438,  ...,  -0.9293,  19.6601,   4.2358],\n",
       "          ...,\n",
       "          [-34.9806,   6.7097, -13.6926,  ..., -23.5921,  14.6618, -58.4223],\n",
       "          [ 24.4547,  17.9597,  -7.6637,  ...,  -5.3158,  32.7607, -13.2856],\n",
       "          [ -1.4969,  13.2782,  36.8608,  ...,  34.2122, -17.9082, -43.4242]]],\n",
       "        device='cuda:0', grad_fn=<UnsafeViewBackward0>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_batch['input_ids'], sample_batch['attention_mask'], sample_batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Clear CUDA cache to free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print memory stats before and after clearing cache\n",
    "    print(f\"GPU memory allocated before clearing cache: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory allocated after clearing cache: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Optional: force garbage collection as well\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input:  revealed to be the “largest climatic and\n",
      "Loss: 436.598267\n",
      "\n",
      "Comparison - Top-k and Top-p Predictions vs Labels:\n",
      "Position 0:\n",
      "  Top-k: 1.' revealed' 2.' pub' 3.'Seg' 4.' patrol' 5.' blatant'\n",
      "  Top-p: nucleus size=0 (p=0.9), sampled=' revealed'\n",
      "  Label: ' to'\n",
      "Position 1:\n",
      "  Top-k: 1.' to' 2.' reps' 3.' Miche' 4.' Ferry' 5.' Bowl'\n",
      "  Top-p: nucleus size=0 (p=0.9), sampled=' to'\n",
      "  Label: ' be'\n",
      "Position 2:\n",
      "  Top-k: 1.' be' 2.' Merc' 3.' landscapes' 4.' onlook' 5.' Brother'\n",
      "  Top-p: nucleus size=0 (p=0.9), sampled=' be'\n",
      "  Label: ' the'\n",
      "Position 3:\n",
      "  Top-k: 1.' the' 2.' evil' 3.'groupon' 4.'Tap' 5.''re'\n",
      "  Top-p: nucleus size=0 (p=0.9), sampled=' the'\n",
      "  Label: ' �'\n",
      "Position 4:\n",
      "  Top-k: 1.' �' 2.' Mack' 3.' kickoff' 4.' Jag' 5.' negativity'\n",
      "  Top-p: nucleus size=0 (p=0.9), sampled=' �'\n",
      "  Label: '�'\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # # After creating the model but before training\n",
    "    # print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    # print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Print input_ids\n",
    "    # print(f\"Input IDs sample: {sample_input_ids[0, :10]}\")  # Print first 10 input IDs of first batch\n",
    "    print(f\"Decoded input: {tokenizer.decode(sample_input_ids[0, :10])}\")  # Decode the first 10 tokens\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(sample_input_ids, labels=sample_labels)\n",
    "    # print(f\"\\nSample batch statistics:\")\n",
    "    # print(f\"Input shape: {sample_input_ids.shape}\")\n",
    "    # print(f\"Labels shape: {sample_labels.shape}\")\n",
    "    # print(f\"Labels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\n",
    "    # print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\n",
    "\n",
    "    print(f\"Loss: {outputs['loss'].item():.6f}\")\n",
    "\n",
    "    # print(f\"\\nLogits shape: {outputs['logits'].shape}\")\n",
    "    # # Print shapes of intermediate outputs\n",
    "    sequence_length = sample_input_ids.size(1)\n",
    "    # vocab_size = outputs['logits'].shape[-1]\n",
    "    # print(f\"Flattened logits shape: {outputs['logits'].view(batch_size * sequence_length, vocab_size).shape}\")\n",
    "    # print(f\"Flattened labels shape: {sample_labels.view(batch_size * sequence_length).shape}\")\n",
    "\n",
    "    # print(f\"Logits mean: {outputs['logits'].mean().item():.6f}\")\n",
    "    # print(f\"Logits std: {outputs['logits'].std().item():.6f}\")\n",
    "    # print(f\"Logits sample: {outputs['logits'][0, 0, :5]}\")  # Print first 5 logits of first token\n",
    "    \n",
    "    # print(f\"\\nTop 3 predicted tokens sample: {topk_indices[0, :10]}\")  # Print first 10 sets of predictions\n",
    "    \n",
    "    # Add top-p (nucleus) sampling\n",
    "    logits = outputs['logits'][0, :10]  # First batch, first 10 positions\n",
    "    top_k = 50\n",
    "    topk_values, topk_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "    softmax_logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    print(f\"\\nComparison - Top-k and Top-p Predictions vs Labels:\")\n",
    "    for i in range(min(5, sequence_length)):\n",
    "        # Top-k results\n",
    "        top_k_tokens = []\n",
    "        for k in range(5):\n",
    "            token = tokenizer.decode(topk_indices[i, k].item())\n",
    "            top_k_tokens.append(f\"{k+1}.'{token}'\")\n",
    "        top_k_str = \" \".join(top_k_tokens)\n",
    "        \n",
    "        # Top-p (nucleus) sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(softmax_logits[i], descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        nucleus_indices = sorted_indices[cumulative_probs <= 0.9]\n",
    "        nucleus_size = len(nucleus_indices)\n",
    "        \n",
    "        # Sample from the nucleus\n",
    "        if nucleus_size > 0:\n",
    "            nucleus_probs = sorted_probs[:nucleus_size]\n",
    "            nucleus_probs = nucleus_probs / nucleus_probs.sum()  # Renormalize probabilities\n",
    "            nucleus_sample_idx = torch.multinomial(nucleus_probs, 1).item()\n",
    "            nucleus_token_idx = nucleus_indices[nucleus_sample_idx].item()\n",
    "            nucleus_token = tokenizer.decode(nucleus_token_idx)\n",
    "        else:\n",
    "            nucleus_token = tokenizer.decode(sorted_indices[0].item())\n",
    "        # Get actual label\n",
    "        label_token = tokenizer.decode(sample_labels[0, i+1].item()) if sample_labels[0, i+1].item() != -100 else \"[MASKED]\"\n",
    "        # Print results\n",
    "        print(f\"Position {i}:\")\n",
    "        # print(f\"  Top-k: 1.'{top1_token}' 2.'{top2_token}' 3.'{top3_token}'\")\n",
    "        print(f\"  Top-k: {top_k_str}\")\n",
    "        print(f\"  Top-p: nucleus size={nucleus_size} (p=0.9), sampled='{nucleus_token}'\")\n",
    "        print(f\"  Label: '{label_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your current debugging cell in roformer_training.ipynb\n",
    "if False:\n",
    "    # Existing initialization checks\n",
    "    print(\"Checking model initialization:\")\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Track intermediate values through the model\n",
    "    with torch.no_grad():\n",
    "        # 1. Check embeddings output\n",
    "        print(\"\\n=== Embeddings Layer ===\")\n",
    "        embedded = model.backbone.embeddings(sample_input_ids)\n",
    "        print(f\"Embeddings output mean: {embedded.mean().item():.6f}\")\n",
    "        print(f\"Embeddings output std: {embedded.std().item():.6f}\")\n",
    "        \n",
    "        # 2. Track through each transformer layer\n",
    "        x = embedded\n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "            print(f\"\\n=== Transformer Layer {i} ===\")\n",
    "            \n",
    "            # 2.1 Self-attention\n",
    "            # Store original input for residual\n",
    "            layer_input = x\n",
    "            \n",
    "            # Get attention outputs\n",
    "            attn_output = layer.self_attn(\n",
    "                q=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                k=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                v=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Attention scores stats:\")\n",
    "            print(f\"  Mean: {attn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {attn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.2 First residual + layer norm\n",
    "            x = layer_input + layer.dropout1(attn_output)\n",
    "            x = layer.ln1(x)\n",
    "            print(f\"After first layer norm:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # 2.3 FFN\n",
    "            ffn_output = layer.ffn(x)\n",
    "            print(f\"FFN output stats:\")\n",
    "            print(f\"  Mean: {ffn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {ffn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.4 Second residual + layer norm\n",
    "            x = x + layer.dropout2(ffn_output)\n",
    "            x = layer.ln2(x)\n",
    "            print(f\"Layer {i} final output:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # Check if output is close to input\n",
    "            similarity = torch.cosine_similarity(layer_input.view(-1), x.view(-1), dim=0)\n",
    "            print(f\"  Cosine similarity with layer input: {similarity.item():.6f}\")\n",
    "        \n",
    "        # 3. Final LM head\n",
    "        print(\"\\n=== LM Head Layer ===\")\n",
    "        logits = model.lm_head(x)\n",
    "        print(f\"Final logits stats:\")\n",
    "        print(f\"  Mean: {logits.mean().item():.6f}\")\n",
    "        print(f\"  Std: {logits.std().item():.6f}\")\n",
    "        \n",
    "        # 4. Check weight tying\n",
    "        print(\"\\n=== Weight Tying Check ===\")\n",
    "        print(f\"Embeddings weight sum: {model.backbone.embeddings.weight.sum().item():.6f}\")\n",
    "        print(f\"LM head weight sum: {model.lm_head.weight.sum().item():.6f}\")\n",
    "        print(f\"Are weights identical? {torch.allclose(model.backbone.embeddings.weight, model.lm_head.weight)}\")\n",
    "        \n",
    "        # 5. Compare predictions with input\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        print(\"\\n=== Input vs Predictions ===\")\n",
    "        print(\"First 5 tokens:\")\n",
    "        for i in range(5):\n",
    "            input_token = tokenizer.decode(sample_input_ids[0, i].item())\n",
    "            pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "            print(f\"Position {i}:\")\n",
    "            print(f\"  Input: '{input_token}'\")\n",
    "            print(f\"  Predicted: '{pred_token}'\")\n",
    "            print(f\"  Token IDs - Input: {sample_input_ids[0, i].item()}, Predicted: {predictions[0, i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer Examples",
   "language": "python",
   "name": "transformer-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
