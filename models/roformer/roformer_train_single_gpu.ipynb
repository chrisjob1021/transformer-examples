{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugpy is already listening: Can't listen for client connections: [Errno 98] Address already in use\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    import debugpy\n",
    "    \n",
    "    # Try to listen on the port, catch exception if already listening\n",
    "    try:\n",
    "        debugpy.listen((\"localhost\", 5678))\n",
    "        print(\"Debugpy is listening on localhost:5678\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Debugpy is already listening: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from utils import TrainingConfig, Config\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # or your custom one\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "training_config = TrainingConfig()\n",
    "config = Config(vocab_size=tokenizer.vocab_size,\n",
    "d_model=768, num_heads=12, ffn_dim=3072,\n",
    "num_layers=12, max_seq_len=tokenizer.model_max_length )\n",
    "\n",
    "savepath = \"/home/chrisobrien/dev/transformer-examples/models/roformer-base\"\n",
    "\n",
    "if False:\n",
    "    # Create a config dictionary for the model\n",
    "    config_dict = {k: getattr(config, k) for k in vars(config) \n",
    "                if not k.startswith('_') and not callable(getattr(config, k))}\n",
    "\n",
    "    # Save the config as JSON\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    with open(os.path.join(savepath, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # 1. Load the raw text\n",
    "    ds = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "    if False:\n",
    "        ds = ds.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    tokenized = load_from_disk(\"gpt2_tokenized_openwebtext\")\n",
    "    tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = tokenized\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=False,\n",
    "            # max_length=config.max_seq_len,\n",
    "            padding=False,\n",
    "            # return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "    tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"], num_proc=24)\n",
    "    # tokenized = tokenized.remove_columns([\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def group_texts(batch):\n",
    "        concatenated = sum(batch[\"input_ids\"], [])\n",
    "        total_length = len(concatenated)\n",
    "        total_length = (total_length // config.max_seq_len) * config.max_seq_len\n",
    "        result = {\n",
    "            \"input_ids\": [concatenated[i:i+config.max_seq_len] for i in range(0, total_length, config.max_seq_len)]\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    lm_dataset = tokenized.remove_columns([\"attention_mask\"])\n",
    "    if False: lm_dataset = lm_dataset.select(range(1000))\n",
    "    lm_dataset = lm_dataset.map(group_texts, batched=True, num_proc=24)\n",
    "    lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"gpt2_tokenized_concatenated_openwebtext\"\n",
    "if False:\n",
    "    # Save the tokenized dataset to disk\n",
    "    lm_dataset.save_to_disk(dataset_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c66d6eb10f4b8a98a6c8989cc44265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    lm_dataset = load_from_disk(dataset_name)\n",
    "    lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 8728153\n",
      "Evaluation dataset size: 88164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 8728153\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 88164\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False: lm_dataset = lm_dataset.select(range(1000))\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_test_split = lm_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Display the datasets\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = lm_dataset\n",
    "    # Let's check a few examples for padding tokens and EOS tokens\n",
    "    for i in range(100):  # Check first 5 examples\n",
    "        # Convert to tensor first, then do the comparison\n",
    "        input_ids = torch.tensor(ds[i][\"input_ids\"])\n",
    "        pad_mask = (input_ids == tokenizer.pad_token_id)\n",
    "        pad_count = pad_mask.sum().item()\n",
    "        \n",
    "        # Check for EOS tokens\n",
    "        eos_mask = (input_ids == tokenizer.eos_token_id)\n",
    "        eos_count = eos_mask.sum().item()\n",
    "        \n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Sequence length: {len(input_ids)}\")\n",
    "        \n",
    "        # Check padding tokens\n",
    "        if pad_count > 0:\n",
    "            print(f\"Pad tokens found: {pad_count}\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"No padding tokens found\")\n",
    "        \n",
    "        # # Check EOS tokens\n",
    "        # if eos_count > 0:\n",
    "        #     print(f\"EOS tokens found: {eos_count}\")\n",
    "        #     # Show where the EOS tokens are\n",
    "        #     eos_positions = (input_ids == tokenizer.eos_token_id).nonzero().flatten().tolist()\n",
    "        #     print(f\"EOS token positions: {eos_positions}\")\n",
    "        # else:\n",
    "        #     print(\"No EOS tokens found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load the tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the tokenized dataset from disk\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "    try:\n",
    "        print(\"Loading tokenized dataset from disk...\")\n",
    "        tokenized = load_from_disk(dataset_name)\n",
    "        print(f\"Successfully loaded tokenized dataset with {len(tokenized)} examples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dataset not found at {dataset_name}. Please make sure you've saved the tokenized dataset first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Save the tokenized dataset to disk\n",
    "    dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "    tokenized.save_to_disk(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Upload to Hugging Face Hub\n",
    "    # You'll need to be logged in to Hugging Face\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # Login to Hugging Face (you'll need to run this once and enter your token)\n",
    "    # Uncomment the line below when you're ready to login\n",
    "    login(\"hf_xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoint-9500\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from models import RoFormerForCausalLM, RoFormerDecoder\n",
    "import torch\n",
    "\n",
    "if False:\n",
    "    # model initialization\n",
    "    model_base = RoFormerDecoder(config)\n",
    "    model = RoFormerForCausalLM(model_base, config)\n",
    "if True: model = RoFormerForCausalLM.from_pretrained(savepath)\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "import os\n",
    "# Get the absolute path for logs\n",
    "log_dir = os.path.join(os.path.dirname(savepath), \"logs\")\n",
    "# Create the logging directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=savepath,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=8, # Accumulate gradients over N steps\n",
    "    #With gradient accumulation (gradient_accumulation_steps=8):\n",
    "        # You split what would have been one batch into 8 smaller micro-batches\n",
    "        # For each micro-batch, you:\n",
    "        # Load 1/8th of the data into memory\n",
    "        # Do a forward pass (storing 1/8th of the activations)\n",
    "        # Do a backward pass (computing 1/8th of the gradients)\n",
    "        # ACCUMULATE the gradients (don't update weights yet)\n",
    "        # Clear the activations (but keep gradients)\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    warmup_steps=100,\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_safetensors=False,\n",
    "    # report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    #With Gradient Checkpointing:\n",
    "        # During the forward pass, only store activations at certain \"checkpoints\"\n",
    "        # During backpropagation, RECOMPUTE the intermediate activations as needed\n",
    "        # This means doing some forward computations twice, but using much less memory\n",
    "    # Without checkpointing, you need to store activations for all 12 layers. With checkpointing, you might only store activations every few layers and recompute the rest during backprop.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    trainer.train(resume_from_checkpoint=savepath + \"/checkpoint-210\")\n",
    "if False:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4602,   284,   307,  ...,    11,   447,   251],\n",
       "        [   25,  3363,    11,  ...,   355,   340, 14051],\n",
       "        [  407,  2245,  6095,  ...,   198,  2394,   475],\n",
       "        ...,\n",
       "        [  287,   477,   604,  ...,    13,    42,  1539],\n",
       "        [ 3529,   198,   198,  ...,    11,   830,  4991],\n",
       "        [ 2242,  4579,    11,  ...,   247,    82,   618]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 4602,   284,   307,  ...,    11,   447,   251],\n",
       "        [   25,  3363,    11,  ...,   355,   340, 14051],\n",
       "        [  407,  2245,  6095,  ...,   198,  2394,   475],\n",
       "        ...,\n",
       "        [  287,   477,   604,  ...,    13,    42,  1539],\n",
       "        [ 3529,   198,   198,  ...,    11,   830,  4991],\n",
       "        [ 2242,  4579,    11,  ...,   247,    82,   618]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor([7.1209], device='cuda:0', grad_fn=<UnsqueezeBackward0>),\n",
       " 'logits': tensor([[[19.6137, 23.6831, 16.1801,  ..., 15.9360, 17.0197, -4.8123],\n",
       "          [21.3259, 21.4489, 16.4488,  ..., 14.6086, 15.4680, -6.0526],\n",
       "          [22.0138, 20.7458, 17.4581,  ..., 14.5760, 14.6444, -3.7428],\n",
       "          ...,\n",
       "          [20.7570, 19.6047, 20.9207,  ..., 14.9131, 14.1257, -6.1593],\n",
       "          [19.8561, 18.9026, 13.8344,  ..., 14.5564, 14.8769, -3.8959],\n",
       "          [18.7335, 19.6889, 16.4301,  ..., 13.0395, 14.5101, -2.6071]],\n",
       " \n",
       "         [[20.1724, 21.0278, 21.0389,  ..., 13.0215, 13.9035, -1.9288],\n",
       "          [23.9106, 20.1374, 19.9415,  ..., 14.5874, 12.5660, -3.5330],\n",
       "          [20.2056, 20.3796, 18.8086,  ..., 13.5624, 14.5789, -4.7895],\n",
       "          ...,\n",
       "          [20.3801, 17.7992, 17.3323,  ..., 14.1799, 12.3988, -4.5551],\n",
       "          [23.7660, 20.1218, 17.1312,  ..., 14.3880, 12.3978, -5.4451],\n",
       "          [23.2175, 20.4138, 17.7706,  ..., 12.9339, 14.7407, -5.8968]],\n",
       " \n",
       "         [[23.2674, 21.0563, 17.7460,  ..., 14.1837, 14.7811, -2.8405],\n",
       "          [20.7396, 19.4397, 17.9913,  ..., 15.2553, 15.3371, -4.4346],\n",
       "          [20.0857, 19.9384, 17.2904,  ..., 14.1928, 13.7704, -4.1667],\n",
       "          ...,\n",
       "          [20.4675, 24.9235, 19.0502,  ..., 16.3151, 12.0783, -4.0156],\n",
       "          [20.9591, 21.2969, 18.4681,  ..., 14.1640, 12.8569, -3.5543],\n",
       "          [19.2255, 19.1217, 16.9878,  ..., 13.6819, 15.5809, -3.4759]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[19.8505, 20.4528, 20.1769,  ..., 17.0402, 14.6491, -3.7783],\n",
       "          [21.0836, 20.3200, 18.5321,  ..., 16.5849, 14.2983, -5.4703],\n",
       "          [19.8744, 20.8136, 19.6210,  ..., 14.7847, 12.5235, -1.3829],\n",
       "          ...,\n",
       "          [21.7675, 18.9341, 16.4244,  ..., 15.1330, 16.2111, -6.2238],\n",
       "          [19.1803, 20.8763, 20.5386,  ..., 14.1184, 13.1759, -4.1709],\n",
       "          [20.6080, 19.7064, 18.4201,  ..., 15.4718, 16.3743, -4.8014]],\n",
       " \n",
       "         [[21.4606, 19.3510, 17.7466,  ..., 15.6474, 16.3167, -3.0359],\n",
       "          [23.7483, 23.4557, 18.0953,  ..., 15.7841, 13.5304, -4.1787],\n",
       "          [21.6700, 21.1475, 19.1427,  ..., 15.3388, 15.5848, -4.1320],\n",
       "          ...,\n",
       "          [20.9246, 19.5872, 16.6823,  ..., 15.3454, 13.7299, -3.6089],\n",
       "          [21.1017, 19.7684, 17.4278,  ..., 15.2068, 15.4750, -4.6129],\n",
       "          [21.3764, 19.4959, 18.3859,  ..., 13.8287, 15.2694, -2.3970]],\n",
       " \n",
       "         [[21.6991, 20.1051, 19.8257,  ..., 12.8773, 12.3703, -3.8763],\n",
       "          [19.0295, 19.5113, 19.0629,  ..., 14.7421, 13.6516,  0.5198],\n",
       "          [19.7495, 20.0842, 17.6273,  ..., 13.5283, 13.6954, -3.7841],\n",
       "          ...,\n",
       "          [21.1333, 19.2048, 18.4151,  ..., 15.3568, 15.5412, -2.8500],\n",
       "          [20.7858, 19.7361, 17.3840,  ..., 15.1365, 14.0579, -4.2220],\n",
       "          [21.7596, 19.0406, 18.5630,  ..., 15.2183, 14.5579, -4.9142]]],\n",
       "        device='cuda:0', grad_fn=<UnsafeViewBackward0>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_batch['input_ids'], sample_batch['attention_mask'], sample_batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Clear CUDA cache to free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print memory stats before and after clearing cache\n",
    "    print(f\"GPU memory allocated before clearing cache: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory allocated after clearing cache: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Optional: force garbage collection as well\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input:  revealed to be the “largest climatic and\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 44.39 GiB of which 1.12 GiB is free. Process 38967 has 524.00 MiB memory in use. Including non-PyTorch memory, this process has 42.75 GiB memory in use. Of the allocated memory 41.88 GiB is allocated by PyTorch, and 390.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDecoded input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer.decode(sample_input_ids[\u001b[32m0\u001b[39m,\u001b[38;5;250m \u001b[39m:\u001b[32m10\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Decode the first 10 tokens\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# print(f\"\\nSample batch statistics:\")\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# print(f\"Input shape: {sample_input_ids.shape}\")\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# print(f\"Labels shape: {sample_labels.shape}\")\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(f\"Labels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m].item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/transformer-examples/models/roformer/roformer.py:181\u001b[39m, in \u001b[36mRoFormerForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels)\u001b[39m\n\u001b[32m    177\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# Flatten the logits and labels for cross entropy loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     shift_logits = \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     shift_labels = labels[:, \u001b[32m1\u001b[39m:].contiguous()\n\u001b[32m    183\u001b[39m     loss = F.cross_entropy(\n\u001b[32m    184\u001b[39m             shift_logits.view(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m    185\u001b[39m             shift_labels.view(-\u001b[32m1\u001b[39m),\n\u001b[32m    186\u001b[39m             ignore_index=-\u001b[32m100\u001b[39m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 44.39 GiB of which 1.12 GiB is free. Process 38967 has 524.00 MiB memory in use. Including non-PyTorch memory, this process has 42.75 GiB memory in use. Of the allocated memory 41.88 GiB is allocated by PyTorch, and 390.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # # After creating the model but before training\n",
    "    # print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    # print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Print input_ids\n",
    "    # print(f\"Input IDs sample: {sample_input_ids[0, :10]}\")  # Print first 10 input IDs of first batch\n",
    "    print(f\"Decoded input: {tokenizer.decode(sample_input_ids[0, :10])}\")  # Decode the first 10 tokens\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(sample_input_ids, labels=sample_labels)\n",
    "    # print(f\"\\nSample batch statistics:\")\n",
    "    # print(f\"Input shape: {sample_input_ids.shape}\")\n",
    "    # print(f\"Labels shape: {sample_labels.shape}\")\n",
    "    # print(f\"Labels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\n",
    "    # print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\n",
    "\n",
    "    print(f\"Loss: {outputs['loss'].item():.6f}\")\n",
    "\n",
    "    # print(f\"\\nLogits shape: {outputs['logits'].shape}\")\n",
    "    # # Print shapes of intermediate outputs\n",
    "    sequence_length = sample_input_ids.size(1)\n",
    "    # vocab_size = outputs['logits'].shape[-1]\n",
    "    # print(f\"Flattened logits shape: {outputs['logits'].view(batch_size * sequence_length, vocab_size).shape}\")\n",
    "    # print(f\"Flattened labels shape: {sample_labels.view(batch_size * sequence_length).shape}\")\n",
    "\n",
    "    # print(f\"Logits mean: {outputs['logits'].mean().item():.6f}\")\n",
    "    # print(f\"Logits std: {outputs['logits'].std().item():.6f}\")\n",
    "    # print(f\"Logits sample: {outputs['logits'][0, 0, :5]}\")  # Print first 5 logits of first token\n",
    "    \n",
    "    # print(f\"\\nTop 3 predicted tokens sample: {topk_indices[0, :10]}\")  # Print first 10 sets of predictions\n",
    "    \n",
    "    # Add top-p (nucleus) sampling\n",
    "    logits = outputs['logits'][0, :10]  # First batch, first 10 positions\n",
    "    top_k = 50\n",
    "    topk_values, topk_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "    softmax_logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    print(f\"\\nComparison - Top-k and Top-p Predictions vs Labels:\")\n",
    "    for i in range(min(5, sequence_length)):\n",
    "        # Top-k results\n",
    "        top_k_tokens = []\n",
    "        for k in range(5):\n",
    "            token = tokenizer.decode(topk_indices[i, k].item())\n",
    "            top_k_tokens.append(f\"{k+1}.'{token}'\")\n",
    "        top_k_str = \" \".join(top_k_tokens)\n",
    "        \n",
    "        # Top-p (nucleus) sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(softmax_logits[i], descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        nucleus_indices = sorted_indices[cumulative_probs <= 0.9]\n",
    "        nucleus_size = len(nucleus_indices)\n",
    "        \n",
    "        # Sample from the nucleus\n",
    "        if nucleus_size > 0:\n",
    "            nucleus_probs = sorted_probs[:nucleus_size]\n",
    "            nucleus_probs = nucleus_probs / nucleus_probs.sum()  # Renormalize probabilities\n",
    "            nucleus_sample_idx = torch.multinomial(nucleus_probs, 1).item()\n",
    "            nucleus_token_idx = nucleus_indices[nucleus_sample_idx].item()\n",
    "            nucleus_token = tokenizer.decode(nucleus_token_idx)\n",
    "        else:\n",
    "            nucleus_token = tokenizer.decode(sorted_indices[0].item())\n",
    "        # Get actual label\n",
    "        label_token = tokenizer.decode(sample_labels[0, i+1].item()) if sample_labels[0, i+1].item() != -100 else \"[MASKED]\"\n",
    "        # Print results\n",
    "        print(f\"Position {i}:\")\n",
    "        # print(f\"  Top-k: 1.'{top1_token}' 2.'{top2_token}' 3.'{top3_token}'\")\n",
    "        print(f\"  Top-k: {top_k_str}\")\n",
    "        print(f\"  Top-p: nucleus size={nucleus_size} (p=0.9), sampled='{nucleus_token}'\")\n",
    "        print(f\"  Label: '{label_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your current debugging cell in roformer_training.ipynb\n",
    "if False:\n",
    "    # Existing initialization checks\n",
    "    print(\"Checking model initialization:\")\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Track intermediate values through the model\n",
    "    with torch.no_grad():\n",
    "        # 1. Check embeddings output\n",
    "        print(\"\\n=== Embeddings Layer ===\")\n",
    "        embedded = model.backbone.embeddings(sample_input_ids)\n",
    "        print(f\"Embeddings output mean: {embedded.mean().item():.6f}\")\n",
    "        print(f\"Embeddings output std: {embedded.std().item():.6f}\")\n",
    "        \n",
    "        # 2. Track through each transformer layer\n",
    "        x = embedded\n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "            print(f\"\\n=== Transformer Layer {i} ===\")\n",
    "            \n",
    "            # 2.1 Self-attention\n",
    "            # Store original input for residual\n",
    "            layer_input = x\n",
    "            \n",
    "            # Get attention outputs\n",
    "            attn_output = layer.self_attn(\n",
    "                q=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                k=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                v=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Attention scores stats:\")\n",
    "            print(f\"  Mean: {attn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {attn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.2 First residual + layer norm\n",
    "            x = layer_input + layer.dropout1(attn_output)\n",
    "            x = layer.ln1(x)\n",
    "            print(f\"After first layer norm:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # 2.3 FFN\n",
    "            ffn_output = layer.ffn(x)\n",
    "            print(f\"FFN output stats:\")\n",
    "            print(f\"  Mean: {ffn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {ffn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.4 Second residual + layer norm\n",
    "            x = x + layer.dropout2(ffn_output)\n",
    "            x = layer.ln2(x)\n",
    "            print(f\"Layer {i} final output:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # Check if output is close to input\n",
    "            similarity = torch.cosine_similarity(layer_input.view(-1), x.view(-1), dim=0)\n",
    "            print(f\"  Cosine similarity with layer input: {similarity.item():.6f}\")\n",
    "        \n",
    "        # 3. Final LM head\n",
    "        print(\"\\n=== LM Head Layer ===\")\n",
    "        logits = model.lm_head(x)\n",
    "        print(f\"Final logits stats:\")\n",
    "        print(f\"  Mean: {logits.mean().item():.6f}\")\n",
    "        print(f\"  Std: {logits.std().item():.6f}\")\n",
    "        \n",
    "        # 4. Check weight tying\n",
    "        print(\"\\n=== Weight Tying Check ===\")\n",
    "        print(f\"Embeddings weight sum: {model.backbone.embeddings.weight.sum().item():.6f}\")\n",
    "        print(f\"LM head weight sum: {model.lm_head.weight.sum().item():.6f}\")\n",
    "        print(f\"Are weights identical? {torch.allclose(model.backbone.embeddings.weight, model.lm_head.weight)}\")\n",
    "        \n",
    "        # 5. Compare predictions with input\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        print(\"\\n=== Input vs Predictions ===\")\n",
    "        print(\"First 5 tokens:\")\n",
    "        for i in range(5):\n",
    "            input_token = tokenizer.decode(sample_input_ids[0, i].item())\n",
    "            pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "            print(f\"Position {i}:\")\n",
    "            print(f\"  Input: '{input_token}'\")\n",
    "            print(f\"  Predicted: '{pred_token}'\")\n",
    "            print(f\"  Token IDs - Input: {sample_input_ids[0, i].item()}, Predicted: {predictions[0, i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer Examples",
   "language": "python",
   "name": "transformer-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
