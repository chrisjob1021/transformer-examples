{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f462c3b61fd44a0a0ed37110e40a060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from utils import TrainingConfig, Config\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")   # or your custom one\n",
    "\n",
    "training_config = TrainingConfig()\n",
    "config = Config(vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768, num_heads=12, ffn_dim=3072,\n",
    "    num_layers=12, max_len=tokenizer.model_max_length )\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load the raw text\n",
    "ds = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = ds.select(range(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 8013769\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=training_config.max_len,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.remove_columns([\"attention_mask\"])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\\n\\nThe decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\\n\\nCNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the Belgian First Aid and Support Team to evacuate. However, Belgian Chief Coordinator Geert Gijs, a doctor who was at the hospital with 60 Belgian medical personnel, said it was his decision to pull the team out for the night. Gijs said he requested U.N. security personnel to staff the hospital overnight, but was told that peacekeepers would only be able to evacuate the team.\\n\\nHe said it was a \"tough decision\" but that he accepted the U.N. offer to evacuate after a Canadian medical team, also at the hospital with Canadian security officers, left the site Friday afternoon. The Belgian team returned Saturday morning.\\n\\nGijs said the United Nations has agreed to provide security for Saturday night. The team has requested the Belgian government to send its own troops for the field hospital, which Gijs expects to arrive late Sunday.\\n\\nResponding to the CNN report that Gupta was the only doctor left at the Port-au-Prince field hospital, U.N. spokesman Martin Nesirky said Saturday that the world body\\'s mission in Haiti did not order any medical team to leave. If the team left, it was at the request of their own organization, he said.\\n\\nEdmond Mulet, the U.N. assistant secretary general for peacekeeping operations, told reporters later that local security officers deemed the makeshift hospital unsafe.\\n\\n\"It seems that we\\'ve heard some reports in the international media that the United Nations asked or forced some medical teams to not work any more in some clinic -- that is not true, that is completely untrue,\" Mulet said Saturday.\\n\\nCNN video from the scene Friday night shows the Belgian team packing up its supplies and leaving with an escort of blue-helmeted U.N. peacekeepers in marked trucks.\\n\\nView or add to CNN\\'s database of missing persons in Haiti\\n\\nGupta -- assisted by other CNN staffers, security personnel and at least one Haitian nurse who refused to leave -- assessed the needs of the 25 patients, but there was little they could do without supplies.\\n\\nMore people, some in critical condition, were trickling in late Friday.\\n\\n\"I\\'ve never been in a situation like this. This is quite ridiculous,\" Gupta said.\\n\\nWith a dearth of medical facilities in Haiti\\'s capital, ambulances had nowhere else to take patients, some of whom had suffered severe trauma -- amputations and head injuries -- under the rubble. Others had suffered a great deal of blood loss, but there were no blood supplies left at the clinic.\\n\\nGupta feared that some would not survive the night.\\n\\nHe and the others stayed with the injured all night, after the medical team had left and after the generators gave out and the tents turned pitch black.\\n\\nGupta monitored patients\\' vital signs, administered painkillers and continued intravenous drips. He stabilized three new patients in critical condition.\\n\\nAt 3:45 a.m., he posted a message on Twitter: \"pulling all nighter at haiti field hosp. lots of work, but all patients stable. turned my crew into a crack med team tonight.\"\\n\\nAre you in Haiti and safe? Share your photos\\n\\nHe said the Belgian doctors did not want to leave their patients behind but were ordered out by the United Nations, which sent buses to transport them.\\n\\n\"There is concern about riots not far from here -- and this is part of the problem,\" Gupta said.\\n\\nThere have been scattered reports of violence throughout the capital.\\n\\n\"What is striking to me as a physician is that patients who just had surgery, patients who are critically ill, are essentially being left here, nobody to care for them,\" Gupta said.\\n\\nSandra Pierre, a Haitian who has been helping at the makeshift hospital, said the medical staff took most of the supplies with them.\\n\\n\"All the doctors, all the nurses are gone,\" she said. \"They are expected to be back tomorrow. They had no plan on leaving tonight. It was an order that came suddenly.\"\\n\\nShe told Gupta, \"It\\'s just you.\"\\n\\nA 7.0 magnitude earthquake flattened Haiti\\'s capital city Tuesday afternoon, affecting as many as 3 million people as it fanned out across the island nation. Tens of thousands of people are feared dead.\\n\\nHaiti, the poorest nation in the Western hemisphere, lacked adequate medical resources even before the disaster and has been struggling this week to tend to huge numbers'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0f8845c30a4fad9a8215917bf4d49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/66 shards):   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the tokenized dataset to disk\n",
    "dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "tokenized.save_to_disk(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Upload to Hugging Face Hub\n",
    "    # You'll need to be logged in to Hugging Face\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # Login to Hugging Face (you'll need to run this once and enter your token)\n",
    "    # Uncomment the line below when you're ready to login\n",
    "    login(\"hf_xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3187b96a3e54dbf9daf07f4074e667c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00001-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e973f0f01a46218422cc626e35c536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00004-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd790e925a1407aa7ed48c889912083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00002-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a4bb05c7bb4cc6bb159eedaa578168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00003-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c369e864513d41778c944581a583c1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b029d27166a64bcda399d98074761ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 66 LFS files:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240b5f002acb447f998a8b3e4e62d718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00005-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20db1c7fa7854294b0652a62c4263c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00006-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e0f19df34e4724a1cd7e16a3c00cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00007-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb62ab48e0364138bfff4e0a6011935c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00008-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2eb030fe814acb8eae0571261e89af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00009-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2db8396c5e48cba0a72eb9cecb5fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00010-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c669db075a441ecb3fb813fcfa1810f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00011-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843a9c7320bb4f368e28612bec10d785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00012-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b960e57cb00a43bba068ec51d2e1977f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00013-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7995bf616d5c415db7678fac1e702ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00014-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5a7951bd50434fb0360206f39f45e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00015-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b98824900114af29826c617c18a8a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00016-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5498830a764bc481a1f16ea4dad0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00017-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6435f0745e4346bbb9ac9213ea312295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00018-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6749554ee1d8405b94b260f25358967e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00019-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d462be5ae04507a8d29475f3c64ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00020-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212df7d201c54003850c5489cfa8254c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00021-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5c079e3aba4afa80e194b4eb5c9df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00022-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db53b31e6384474a803c9da6779fc3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00023-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9afe2f7c864621a6bed4a8b2b48a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00024-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e5d1c527124fa0a89260a06af10f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00025-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c947f4eac8d240a0963eb131eca968dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00026-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5a45c952854c8e93c5bb057a5f608e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00027-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaf662cf8834a1b80a92368199c7307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00028-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc50d20e793d49deab638c5642d76899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00029-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ac43ad74aa4572824a606dd3d1c81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00030-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475c7e2149594a58b41d740a38679391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00031-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ae6223aee6459a9317563f72280432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00032-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fea522bf3c74359bf80d8455860325b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00033-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e821b7b0e1234862a8a78246a6926af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00034-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866afa8290c2463eaa1e62326cf884a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00035-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdeec2e7d854c5fbd5263b03ad8dc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00036-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419303848ee8428f9a33fd6fe41a9b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00037-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6f98f8d36645788e7844dcffac6dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00038-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52d12d16b9844a6a61f9cfdb1c93684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00039-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc5ab6cf0e04f40bd4852dfa0bee934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00040-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cb3045e0d046fa8b3f0a832f6a9930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00041-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c848f61bcfd4da989ed4d9df9d9aa72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00042-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0bda6b8363417e99b4715f150f4ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00043-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af00db447894cb6bcabeed34dbb7def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00044-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584d697b4c0649659f1f3791acbaad69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00045-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2ecf86e92b4dbd91f59dd07d7c1bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00046-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30a44284dff4aa99ea423ed1989da83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00047-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7331a5e5a3e94cb7bf92c0963d612123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00048-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eebeab03238488b81bd6ba3d374acc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00049-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512fc844e6ef441da8b711058679a9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00050-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bfe1ee383e4f0398ae75b3237dedac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00051-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e7eea77a2c43759558c7170364dce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00052-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9802ecf75fe64789a53834df346122bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00053-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69870cabb8f64c57be22dae39a7e5fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00054-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522f41c3741148739d28ec12a1249b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00055-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f66cdf748d437ba2ae7900d789342c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00056-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd89d8b5ffb4c419fb2f1f5249ad29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00057-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf44e4216c0049ab9fdb971cf61fb94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00058-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74349e518a941328b1d764836d3c2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00059-of-00066.arrow:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    # Save the tokenized dataset to disk\n",
    "    dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "    username = \"chrisjob1021\"\n",
    "\n",
    "    # Upload to Hugging Face Hub\n",
    "    # You'll need to be logged in to Hugging Face\n",
    "    from huggingface_hub import HfApi\n",
    "\n",
    "    # Initialize the Hugging Face API\n",
    "    api = HfApi()\n",
    "\n",
    "    # Upload the dataset to the Hub\n",
    "    # Replace \"your-username/tokenized-openwebtext\" with your desired repository name\n",
    "    try:\n",
    "        api.create_repo(\n",
    "            repo_id=username + \"/\" + dataset_name,\n",
    "            repo_type=\"dataset\",\n",
    "            exist_ok=True\n",
    "        )\n",
    "        \n",
    "        api.upload_folder(\n",
    "            folder_path=dataset_name,\n",
    "            repo_id=username + \"/\" + dataset_name,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        \n",
    "        print(\"Dataset successfully uploaded to Hugging Face Hub!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading dataset: {e}\")\n",
    "        print(\"You may need to login first with `login()` or check your permissions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def group(batch):\n",
    "#     # Flattens the input_ids and attention_mask into single lists\n",
    "#     flat_ids = sum(batch[\"input_ids\"], [])\n",
    "\n",
    "#     num_of_complete_blocks = len(flat_ids) // config.max_seq_len\n",
    "#     total = num_of_complete_blocks * config.max_seq_len\n",
    "#     flat_ids = flat_ids[:total+1]\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids\": [flat_ids[i:i+config.max_seq_len] for i in range(0, total, config.max_seq_len)],\n",
    "#         \"labels\": [flat_ids[i+1:i+config.max_seq_len+1] for i in range(0, total, config.max_seq_len)]\n",
    "#     }\n",
    "\n",
    "\n",
    "# # lm_ds = tokenized.map(group, batched=True, batch_size=10000)\n",
    "# # lm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RoFormerEncoder, RoFormerForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "model_base = RoFormerEncoder(config)\n",
    "model = RoFormerForCausalLM(model_base, config)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"roformer-base\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8, # Accumulate gradients over N steps\n",
    "    #With gradient accumulation (gradient_accumulation_steps=8):\n",
    "        # You split what would have been one batch into 8 smaller micro-batches\n",
    "        # For each micro-batch, you:\n",
    "        # Load 1/8th of the data into memory\n",
    "        # Do a forward pass (storing 1/8th of the activations)\n",
    "        # Do a backward pass (computing 1/8th of the gradients)\n",
    "        # ACCUMULATE the gradients (don't update weights yet)\n",
    "        # Clear the activations (but keep gradients)\n",
    "    \n",
    "    warmup_steps=10,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_safetensors=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    #With Gradient Checkpointing:\n",
    "        # During the forward pass, only store activations at certain \"checkpoints\"\n",
    "        # During backpropagation, RECOMPUTE the intermediate activations as needed\n",
    "        # This means doing some forward computations twice, but using much less memory\n",
    "    # Without checkpointing, you need to store activations for all 12 layers. With checkpointing, you might only store activations every few layers and recompute the rest during backprop.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=lm_ds,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  31/9375 00:59 < 5:17:04, 0.49 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>425.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>141.448100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:2627\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2626\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2627\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2638\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3211\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3207\u001b[39m         \u001b[38;5;28mself\u001b[39m.state.best_model_checkpoint = best_checkpoint_dir\n\u001b[32m   3209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   3210\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3211\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3212\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_scaler(output_dir)\n\u001b[32m   3213\u001b[39m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3339\u001b[39m, in \u001b[36mTrainer._save_optimizer_and_scheduler\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m   3334\u001b[39m     save_fsdp_optimizer(\n\u001b[32m   3335\u001b[39m         \u001b[38;5;28mself\u001b[39m.accelerator.state.fsdp_plugin, \u001b[38;5;28mself\u001b[39m.accelerator, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.model, output_dir\n\u001b[32m   3336\u001b[39m     )\n\u001b[32m   3337\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m   3338\u001b[39m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3339\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3341\u001b[39m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[32m   3342\u001b[39m is_deepspeed_custom_scheduler = \u001b[38;5;28mself\u001b[39m.is_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   3343\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[32m   3344\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/serialization.py:1216\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1214\u001b[39m     storage = storage.cpu()\n\u001b[32m   1215\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 374])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "sample_batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding weight mean: 0.000124\n",
      "Embedding weight std: 1.000025\n",
      "Input IDs sample: tensor([  40,  447,  247,   76,  407, 3375,  546,  406, 4720,   82],\n",
      "       device='cuda:0')\n",
      "Decoded input: I’m not talking about LEOs\n",
      "Labels sample: tensor([  40,  447,  247,   76,  407, 3375,  546,  406, 4720,   82],\n",
      "       device='cuda:0')\n",
      "Decoded labels: I’m not talking about LEOs\n",
      "\n",
      "Loss: 483.990692\n",
      "\n",
      "Predicted classes sample: tensor([  40,  447,  247,   76,  407, 3375,  546,  406, 4720,   82],\n",
      "       device='cuda:0')\n",
      "Comparison - Predictions vs Labels:\n",
      "Position 0: Predicted 'I' | Label '�'\n",
      "Position 1: Predicted '�' | Label '�'\n",
      "Position 2: Predicted '�' | Label 'm'\n",
      "Position 3: Predicted 'm' | Label ' not'\n",
      "Position 4: Predicted ' not' | Label ' talking'\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # After creating the model but before training\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Print input_ids\n",
    "    print(f\"Input IDs sample: {sample_input_ids[0, :10]}\")  # Print first 10 input IDs of first batch\n",
    "    print(f\"Decoded input: {tokenizer.decode(sample_input_ids[0, :10])}\")  # Decode the first 10 tokens\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(sample_input_ids, labels=sample_labels)\n",
    "    # print(f\"\\nSample batch statistics:\")\n",
    "    # print(f\"Input shape: {sample_input_ids.shape}\")\n",
    "    # print(f\"Labels shape: {sample_labels.shape}\")\n",
    "    print(f\"Labels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\n",
    "    print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\n",
    "\n",
    "    print(f\"\\nLoss: {outputs['loss'].item():.6f}\")\n",
    "\n",
    "    # print(f\"\\nLogits shape: {outputs['logits'].shape}\")\n",
    "    # # Print shapes of intermediate outputs\n",
    "    sequence_length = sample_input_ids.size(1)\n",
    "    # vocab_size = outputs['logits'].shape[-1]\n",
    "    # print(f\"Flattened logits shape: {outputs['logits'].view(batch_size * sequence_length, vocab_size).shape}\")\n",
    "    # print(f\"Flattened labels shape: {sample_labels.view(batch_size * sequence_length).shape}\")\n",
    "\n",
    "    # print(f\"Logits mean: {outputs['logits'].mean().item():.6f}\")\n",
    "    # print(f\"Logits std: {outputs['logits'].std().item():.6f}\")\n",
    "    # print(f\"Logits sample: {outputs['logits'][0, 0, :5]}\")  # Print first 5 logits of first token\n",
    "    \n",
    "    # Get predicted classes from logits\n",
    "    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "    print(f\"\\nPredicted classes sample: {predictions[0, :10]}\")  # Print first 10 predicted tokens\n",
    "    print(f\"Comparison - Predictions vs Labels:\")\n",
    "    for i in range(min(5, sequence_length)):\n",
    "        pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "        label_token = tokenizer.decode(sample_labels[0, i+1].item()) if sample_labels[0, i+1].item() != -100 else \"[MASKED]\"\n",
    "        print(f\"Position {i}: Predicted '{pred_token}' | Label '{label_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model initialization:\n",
      "Embedding weight mean: -0.000035\n",
      "Embedding weight std: 0.999916\n",
      "\n",
      "=== Embeddings Layer ===\n",
      "Embeddings output mean: -0.001582\n",
      "Embeddings output std: 0.998326\n",
      "\n",
      "=== Transformer Layer 0 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.003008\n",
      "  Std: 0.066774\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.012456\n",
      "  Std: 0.233869\n",
      "Layer 0 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cosine similarity with layer input: 0.967763\n",
      "\n",
      "=== Transformer Layer 1 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000862\n",
      "  Std: 0.080982\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.012353\n",
      "  Std: 0.236669\n",
      "Layer 1 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.967129\n",
      "\n",
      "=== Transformer Layer 2 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000418\n",
      "  Std: 0.097013\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.001413\n",
      "  Std: 0.234197\n",
      "Layer 2 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.965975\n",
      "\n",
      "=== Transformer Layer 3 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.002001\n",
      "  Std: 0.106930\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.001594\n",
      "  Std: 0.236986\n",
      "Layer 3 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.965236\n",
      "\n",
      "=== Transformer Layer 4 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.004253\n",
      "  Std: 0.115905\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.001687\n",
      "  Std: 0.232923\n",
      "Layer 4 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.964138\n",
      "\n",
      "=== Transformer Layer 5 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.008181\n",
      "  Std: 0.126545\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.000831\n",
      "  Std: 0.237432\n",
      "Layer 5 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.961067\n",
      "\n",
      "=== Transformer Layer 6 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.002946\n",
      "  Std: 0.138081\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.007111\n",
      "  Std: 0.234475\n",
      "Layer 6 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.961118\n",
      "\n",
      "=== Transformer Layer 7 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.007337\n",
      "  Std: 0.152214\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.006803\n",
      "  Std: 0.238030\n",
      "Layer 7 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.958343\n",
      "\n",
      "=== Transformer Layer 8 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.003132\n",
      "  Std: 0.161186\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.009883\n",
      "  Std: 0.236669\n",
      "Layer 8 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.957122\n",
      "\n",
      "=== Transformer Layer 9 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.005780\n",
      "  Std: 0.181597\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.004612\n",
      "  Std: 0.236220\n",
      "Layer 9 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.953346\n",
      "\n",
      "=== Transformer Layer 10 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.001344\n",
      "  Std: 0.178087\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.000075\n",
      "  Std: 0.231150\n",
      "Layer 10 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.954064\n",
      "\n",
      "=== Transformer Layer 11 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.002484\n",
      "  Std: 0.197431\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.003498\n",
      "  Std: 0.228389\n",
      "Layer 11 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.951855\n",
      "\n",
      "=== LM Head Layer ===\n",
      "Final logits stats:\n",
      "  Mean: 0.149523\n",
      "  Std: 27.796478\n",
      "\n",
      "=== Weight Tying Check ===\n",
      "Embeddings weight sum: -1343.704224\n",
      "LM head weight sum: -1343.704224\n",
      "Are weights identical? True\n",
      "\n",
      "=== Input vs Predictions ===\n",
      "First 5 tokens:\n",
      "Position 0:\n",
      "  Input: ' the'\n",
      "  Predicted: ' the'\n",
      "  Token IDs - Input: 262, Predicted: 262\n",
      "Position 1:\n",
      "  Input: ' class'\n",
      "  Predicted: ' class'\n",
      "  Token IDs - Input: 1398, Predicted: 1398\n",
      "Position 2:\n",
      "  Input: ' of'\n",
      "  Predicted: ' of'\n",
      "  Token IDs - Input: 286, Predicted: 286\n",
      "Position 3:\n",
      "  Input: ' academic'\n",
      "  Predicted: ' academic'\n",
      "  Token IDs - Input: 8233, Predicted: 8233\n",
      "Position 4:\n",
      "  Input: ' managers'\n",
      "  Predicted: ' managers'\n",
      "  Token IDs - Input: 11663, Predicted: 11663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model initialization:\n",
      "Embedding weight mean: 0.000371\n",
      "Embedding weight std: 0.993480\n",
      "\n",
      "=== Embeddings Layer ===\n",
      "Embeddings output mean: -0.006669\n",
      "Embeddings output std: 0.988082\n",
      "\n",
      "=== Transformer Layer 0 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.001654\n",
      "  Std: 0.086608\n",
      "After first layer norm:\n",
      "  Mean: -0.000705\n",
      "  Std: 1.005569\n",
      "FFN output stats:\n",
      "  Mean: -0.243816\n",
      "  Std: 1.829514\n",
      "Layer 0 final output:\n",
      "  Mean: -0.005556\n",
      "  Std: 1.048462\n",
      "  Cosine similarity with layer input: 0.439574\n",
      "\n",
      "=== Transformer Layer 1 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.002548\n",
      "  Std: 0.063809\n",
      "After first layer norm:\n",
      "  Mean: -0.005927\n",
      "  Std: 1.049744\n",
      "FFN output stats:\n",
      "  Mean: -0.334229\n",
      "  Std: 2.327831\n",
      "Layer 1 final output:\n",
      "  Mean: -0.007043\n",
      "  Std: 1.056500\n",
      "  Cosine similarity with layer input: 0.757285\n",
      "\n",
      "=== Transformer Layer 2 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000382\n",
      "  Std: 0.021672\n",
      "After first layer norm:\n",
      "  Mean: -0.003490\n",
      "  Std: 1.026778\n",
      "FFN output stats:\n",
      "  Mean: -0.030357\n",
      "  Std: 0.270173\n",
      "Layer 2 final output:\n",
      "  Mean: -0.006080\n",
      "  Std: 1.064491\n",
      "  Cosine similarity with layer input: 0.965322\n",
      "\n",
      "=== Transformer Layer 3 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000517\n",
      "  Std: 0.019466\n",
      "After first layer norm:\n",
      "  Mean: -0.003582\n",
      "  Std: 1.047384\n",
      "FFN output stats:\n",
      "  Mean: -0.006529\n",
      "  Std: 0.085088\n",
      "Layer 3 final output:\n",
      "  Mean: -0.003390\n",
      "  Std: 1.051041\n",
      "  Cosine similarity with layer input: 0.990341\n",
      "\n",
      "=== Transformer Layer 4 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000876\n",
      "  Std: 0.017126\n",
      "After first layer norm:\n",
      "  Mean: -0.002243\n",
      "  Std: 1.045167\n",
      "FFN output stats:\n",
      "  Mean: -0.002636\n",
      "  Std: 0.044266\n",
      "Layer 4 final output:\n",
      "  Mean: -0.001885\n",
      "  Std: 1.043301\n",
      "  Cosine similarity with layer input: 0.995249\n",
      "\n",
      "=== Transformer Layer 5 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000769\n",
      "  Std: 0.015076\n",
      "After first layer norm:\n",
      "  Mean: -0.001370\n",
      "  Std: 1.040751\n",
      "FFN output stats:\n",
      "  Mean: -0.001076\n",
      "  Std: 0.029577\n",
      "Layer 5 final output:\n",
      "  Mean: -0.001198\n",
      "  Std: 1.038042\n",
      "  Cosine similarity with layer input: 0.997239\n",
      "\n",
      "=== Transformer Layer 6 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000159\n",
      "  Std: 0.013491\n",
      "After first layer norm:\n",
      "  Mean: -0.000964\n",
      "  Std: 1.036912\n",
      "FFN output stats:\n",
      "  Mean: -0.000433\n",
      "  Std: 0.021765\n",
      "Layer 6 final output:\n",
      "  Mean: -0.000747\n",
      "  Std: 1.033963\n",
      "  Cosine similarity with layer input: 0.998204\n",
      "\n",
      "=== Transformer Layer 7 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000038\n",
      "  Std: 0.012309\n",
      "After first layer norm:\n",
      "  Mean: -0.000633\n",
      "  Std: 1.033324\n",
      "FFN output stats:\n",
      "  Mean: -0.000145\n",
      "  Std: 0.017137\n",
      "Layer 7 final output:\n",
      "  Mean: -0.000630\n",
      "  Std: 1.030371\n",
      "  Cosine similarity with layer input: 0.998760\n",
      "\n",
      "=== Transformer Layer 8 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000261\n",
      "  Std: 0.011434\n",
      "After first layer norm:\n",
      "  Mean: -0.000520\n",
      "  Std: 1.030084\n",
      "FFN output stats:\n",
      "  Mean: -0.000302\n",
      "  Std: 0.013857\n",
      "Layer 8 final output:\n",
      "  Mean: -0.000476\n",
      "  Std: 1.027056\n",
      "  Cosine similarity with layer input: 0.999087\n",
      "\n",
      "=== Transformer Layer 9 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000025\n",
      "  Std: 0.010115\n",
      "After first layer norm:\n",
      "  Mean: -0.000425\n",
      "  Std: 1.026949\n",
      "FFN output stats:\n",
      "  Mean: -0.000423\n",
      "  Std: 0.011531\n",
      "Layer 9 final output:\n",
      "  Mean: -0.000348\n",
      "  Std: 1.023858\n",
      "  Cosine similarity with layer input: 0.999376\n",
      "\n",
      "=== Transformer Layer 10 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000147\n",
      "  Std: 0.009684\n",
      "After first layer norm:\n",
      "  Mean: -0.000326\n",
      "  Std: 1.023915\n",
      "FFN output stats:\n",
      "  Mean: -0.000301\n",
      "  Std: 0.009736\n",
      "Layer 10 final output:\n",
      "  Mean: -0.000233\n",
      "  Std: 1.020641\n",
      "  Cosine similarity with layer input: 0.999546\n",
      "\n",
      "=== Transformer Layer 11 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000170\n",
      "  Std: 0.008808\n",
      "After first layer norm:\n",
      "  Mean: -0.000212\n",
      "  Std: 1.020770\n",
      "FFN output stats:\n",
      "  Mean: -0.000002\n",
      "  Std: 0.008448\n",
      "Layer 11 final output:\n",
      "  Mean: 0.003712\n",
      "  Std: 0.896938\n",
      "  Cosine similarity with layer input: 0.998598\n",
      "\n",
      "=== LM Head Layer ===\n",
      "Final logits stats:\n",
      "  Mean: 5.884747\n",
      "  Std: 5.349260\n",
      "\n",
      "=== Weight Tying Check ===\n",
      "Embeddings weight sum: 14306.712891\n",
      "LM head weight sum: 14306.712891\n",
      "Are weights identical? True\n",
      "\n",
      "=== Input vs Predictions ===\n",
      "First 5 tokens:\n",
      "Position 0:\n",
      "  Input: 'The'\n",
      "  Predicted: ' an'\n",
      "  Token IDs - Input: 464, Predicted: 281\n",
      "Position 1:\n",
      "  Input: ' biggest'\n",
      "  Predicted: ' and'\n",
      "  Token IDs - Input: 4094, Predicted: 290\n",
      "Position 2:\n",
      "  Input: ' opportunities'\n",
      "  Predicted: ' for'\n",
      "  Token IDs - Input: 6443, Predicted: 329\n",
      "Position 3:\n",
      "  Input: ' for'\n",
      "  Predicted: ' the'\n",
      "  Token IDs - Input: 329, Predicted: 262\n",
      "Position 4:\n",
      "  Input: ' bitcoin'\n",
      "  Predicted: '�'\n",
      "  Token IDs - Input: 8550, Predicted: 447\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Add this to your current debugging cell in roformer_training.ipynb\n",
    "if True:\n",
    "    # Existing initialization checks\n",
    "    print(\"Checking model initialization:\")\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Track intermediate values through the model\n",
    "    with torch.no_grad():\n",
    "        # 1. Check embeddings output\n",
    "        print(\"\\n=== Embeddings Layer ===\")\n",
    "        embedded = model.backbone.embeddings(sample_input_ids)\n",
    "        print(f\"Embeddings output mean: {embedded.mean().item():.6f}\")\n",
    "        print(f\"Embeddings output std: {embedded.std().item():.6f}\")\n",
    "        \n",
    "        # 2. Track through each transformer layer\n",
    "        x = embedded\n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "            print(f\"\\n=== Transformer Layer {i} ===\")\n",
    "            \n",
    "            # 2.1 Self-attention\n",
    "            # Store original input for residual\n",
    "            layer_input = x\n",
    "            \n",
    "            # Get attention outputs\n",
    "            attn_output = layer.self_attn(\n",
    "                q=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                k=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                v=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Attention scores stats:\")\n",
    "            print(f\"  Mean: {attn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {attn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.2 First residual + layer norm\n",
    "            x = layer_input + layer.dropout1(attn_output)\n",
    "            x = layer.ln1(x)\n",
    "            print(f\"After first layer norm:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # 2.3 FFN\n",
    "            ffn_output = layer.ffn(x)\n",
    "            print(f\"FFN output stats:\")\n",
    "            print(f\"  Mean: {ffn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {ffn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.4 Second residual + layer norm\n",
    "            x = x + layer.dropout2(ffn_output)\n",
    "            x = layer.ln2(x)\n",
    "            print(f\"Layer {i} final output:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # Check if output is close to input\n",
    "            similarity = torch.cosine_similarity(layer_input.view(-1), x.view(-1), dim=0)\n",
    "            print(f\"  Cosine similarity with layer input: {similarity.item():.6f}\")\n",
    "        \n",
    "        # 3. Final LM head\n",
    "        print(\"\\n=== LM Head Layer ===\")\n",
    "        logits = model.lm_head(x)\n",
    "        print(f\"Final logits stats:\")\n",
    "        print(f\"  Mean: {logits.mean().item():.6f}\")\n",
    "        print(f\"  Std: {logits.std().item():.6f}\")\n",
    "        \n",
    "        # 4. Check weight tying\n",
    "        print(\"\\n=== Weight Tying Check ===\")\n",
    "        print(f\"Embeddings weight sum: {model.backbone.embeddings.weight.sum().item():.6f}\")\n",
    "        print(f\"LM head weight sum: {model.lm_head.weight.sum().item():.6f}\")\n",
    "        print(f\"Are weights identical? {torch.allclose(model.backbone.embeddings.weight, model.lm_head.weight)}\")\n",
    "        \n",
    "        # 5. Compare predictions with input\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        print(\"\\n=== Input vs Predictions ===\")\n",
    "        print(\"First 5 tokens:\")\n",
    "        for i in range(5):\n",
    "            input_token = tokenizer.decode(sample_input_ids[0, i].item())\n",
    "            pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "            print(f\"Position {i}:\")\n",
    "            print(f\"  Input: '{input_token}'\")\n",
    "            print(f\"  Predicted: '{pred_token}'\")\n",
    "            print(f\"  Token IDs - Input: {sample_input_ids[0, i].item()}, Predicted: {predictions[0, i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer Examples",
   "language": "python",
   "name": "transformer-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
