{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454ee351316f42239ad7127c2079951c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from utils import TrainingConfig, Config\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")   # or your custom one\n",
    "\n",
    "training_config = TrainingConfig()\n",
    "config = Config(vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768, num_heads=12, ffn_dim=3072,\n",
    "    num_layers=12, )\n",
    "\n",
    "# 1. Load the raw text\n",
    "ds = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    ds = ds.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        # truncation=False,\n",
    "        # max_length=training_config.max_len,\n",
    "        # padding=False,\n",
    "        # return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.remove_columns([\"attention_mask\"])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 1099\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group(batch):\n",
    "    # Flattens the input_ids and attention_mask into single lists\n",
    "    flat_ids = sum(batch[\"input_ids\"], [])\n",
    "\n",
    "    num_of_complete_blocks = len(flat_ids) // config.max_seq_len\n",
    "    total = num_of_complete_blocks * config.max_seq_len\n",
    "    flat_ids = flat_ids[:total+1]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": [flat_ids[i:i+config.max_seq_len] for i in range(0, total, config.max_seq_len)],\n",
    "        \"labels\": [flat_ids[i+1:i+config.max_seq_len+1] for i in range(0, total, config.max_seq_len)]\n",
    "    }\n",
    "\n",
    "\n",
    "lm_ds = tokenized.map(group, batched=True, batch_size=10000)\n",
    "lm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from models import RoFormerEncoder, RoFormerForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "model_base = RoFormerEncoder(config)\n",
    "model = RoFormerForCausalLM(model_base, config)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"roformer-base\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=10,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_safetensors=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=lm_ds,\n",
    "    # data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)  # Should print 'cuda:0' or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model initialization:\n",
      "Embedding weight mean: 0.000213\n",
      "Embedding weight std: 1.000068\n",
      "\n",
      "Input IDs sample: tensor([  262,  1398,   286,  8233, 11663,  4497,   329,   262,  5876,   287],\n",
      "       device='cuda:0')\n",
      "Decoded input:  the class of academic managers responsible for the trouble in\n",
      "\n",
      "Labels sample: tensor([ 1398,   286,  8233, 11663,  4497,   329,   262,  5876,   287,   262],\n",
      "       device='cuda:0')\n",
      "Decoded labels:  class of academic managers responsible for the trouble in the\n",
      "\n",
      "Loss: 497.880768\n",
      "\n",
      "Logits shape: torch.Size([4, 1024, 50257])\n",
      "Flattened logits shape: torch.Size([4096, 50257])\n",
      "Flattened labels shape: torch.Size([4096])\n",
      "Logits mean: 0.115224\n",
      "Logits std: 27.812685\n",
      "Logits sample: tensor([-13.6797, -12.5401,  -1.9143,  16.7642,  44.0470], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Predicted classes sample: tensor([  262,  1398,   286,  8233, 11663,  4497,   329,   262,  5876,   287],\n",
      "       device='cuda:0')\n",
      "Comparison - Predictions vs Labels:\n",
      "Position 0: Predicted ' the' | Label ' class'\n",
      "Position 1: Predicted ' class' | Label ' of'\n",
      "Position 2: Predicted ' of' | Label ' academic'\n",
      "Position 3: Predicted ' academic' | Label ' managers'\n",
      "Position 4: Predicted ' managers' | Label ' responsible'\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # After creating the model but before training\n",
    "    print(\"Checking model initialization:\")\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Print input_ids\n",
    "    print(f\"\\nInput IDs sample: {sample_input_ids[0, :10]}\")  # Print first 10 input IDs of first batch\n",
    "    print(f\"Decoded input: {tokenizer.decode(sample_input_ids[0, :10])}\")  # Decode the first 10 tokens\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(sample_input_ids, labels=sample_labels)\n",
    "    # print(f\"\\nSample batch statistics:\")\n",
    "    # print(f\"Input shape: {sample_input_ids.shape}\")\n",
    "    # print(f\"Labels shape: {sample_labels.shape}\")\n",
    "    print(f\"\\nLabels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\n",
    "    print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\n",
    "\n",
    "    print(f\"\\nLoss: {outputs['loss'].item():.6f}\")\n",
    "\n",
    "    print(f\"\\nLogits shape: {outputs['logits'].shape}\")\n",
    "    \n",
    "    # Print shapes of intermediate outputs\n",
    "    batch_size, sequence_length = sample_input_ids.shape\n",
    "    vocab_size = outputs['logits'].shape[-1]\n",
    "    print(f\"Flattened logits shape: {outputs['logits'].view(batch_size * sequence_length, vocab_size).shape}\")\n",
    "    print(f\"Flattened labels shape: {sample_labels.view(batch_size * sequence_length).shape}\")\n",
    "\n",
    "    print(f\"Logits mean: {outputs['logits'].mean().item():.6f}\")\n",
    "    print(f\"Logits std: {outputs['logits'].std().item():.6f}\")\n",
    "    print(f\"Logits sample: {outputs['logits'][0, 0, :5]}\")  # Print first 5 logits of first token\n",
    "    \n",
    "    # Get predicted classes from logits\n",
    "    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "    print(f\"\\nPredicted classes sample: {predictions[0, :10]}\")  # Print first 10 predicted tokens\n",
    "    print(f\"Comparison - Predictions vs Labels:\")\n",
    "    for i in range(min(5, sequence_length)):\n",
    "        pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "        label_token = tokenizer.decode(sample_labels[0, i].item()) if sample_labels[0, i].item() != -100 else \"[MASKED]\"\n",
    "        print(f\"Position {i}: Predicted '{pred_token}' | Label '{label_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model initialization:\n",
      "Embedding weight mean: 0.000213\n",
      "Embedding weight std: 1.000068\n",
      "\n",
      "=== Embeddings Layer ===\n",
      "Embeddings output mean: 0.004081\n",
      "Embeddings output std: 0.996310\n",
      "\n",
      "=== Transformer Layer 0 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000157\n",
      "  Std: 0.024678\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.004629\n",
      "  Std: 0.231079\n",
      "Layer 0 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.970191\n",
      "\n",
      "=== Transformer Layer 1 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.001262\n",
      "  Std: 0.027172\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.007538\n",
      "  Std: 0.238840\n",
      "Layer 1 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.969486\n",
      "\n",
      "=== Transformer Layer 2 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000579\n",
      "  Std: 0.029659\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.001685\n",
      "  Std: 0.234047\n",
      "Layer 2 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.970362\n",
      "\n",
      "=== Transformer Layer 3 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000043\n",
      "  Std: 0.032545\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.003957\n",
      "  Std: 0.237582\n",
      "Layer 3 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.969717\n",
      "\n",
      "=== Transformer Layer 4 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.001896\n",
      "  Std: 0.033930\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.002703\n",
      "  Std: 0.235907\n",
      "Layer 4 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.969780\n",
      "\n",
      "=== Transformer Layer 5 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.001354\n",
      "  Std: 0.035050\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.003387\n",
      "  Std: 0.238616\n",
      "Layer 5 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.968897\n",
      "\n",
      "=== Transformer Layer 6 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.001633\n",
      "  Std: 0.037304\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.000928\n",
      "  Std: 0.232878\n",
      "Layer 6 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.970556\n",
      "\n",
      "=== Transformer Layer 7 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000306\n",
      "  Std: 0.038942\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.001173\n",
      "  Std: 0.237525\n",
      "Layer 7 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.969237\n",
      "\n",
      "=== Transformer Layer 8 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.001308\n",
      "  Std: 0.041861\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.008372\n",
      "  Std: 0.234939\n",
      "Layer 8 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.969904\n",
      "\n",
      "=== Transformer Layer 9 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000120\n",
      "  Std: 0.040142\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.004982\n",
      "  Std: 0.225806\n",
      "Layer 9 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.971937\n",
      "\n",
      "=== Transformer Layer 10 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.002406\n",
      "  Std: 0.042388\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.001558\n",
      "  Std: 0.236788\n",
      "Layer 10 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.969505\n",
      "\n",
      "=== Transformer Layer 11 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000463\n",
      "  Std: 0.043865\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.002595\n",
      "  Std: 0.237043\n",
      "Layer 11 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.969245\n",
      "\n",
      "=== LM Head Layer ===\n",
      "Final logits stats:\n",
      "  Mean: 0.113476\n",
      "  Std: 27.817223\n",
      "\n",
      "=== Weight Tying Check ===\n",
      "Embeddings weight sum: 8237.390625\n",
      "LM head weight sum: 8237.390625\n",
      "Are weights identical? True\n",
      "\n",
      "=== Input vs Predictions ===\n",
      "First 5 tokens:\n",
      "Position 0:\n",
      "  Input: ' the'\n",
      "  Predicted: ' the'\n",
      "  Token IDs - Input: 262, Predicted: 262\n",
      "Position 1:\n",
      "  Input: ' class'\n",
      "  Predicted: ' class'\n",
      "  Token IDs - Input: 1398, Predicted: 1398\n",
      "Position 2:\n",
      "  Input: ' of'\n",
      "  Predicted: ' of'\n",
      "  Token IDs - Input: 286, Predicted: 286\n",
      "Position 3:\n",
      "  Input: ' academic'\n",
      "  Predicted: ' academic'\n",
      "  Token IDs - Input: 8233, Predicted: 8233\n",
      "Position 4:\n",
      "  Input: ' managers'\n",
      "  Predicted: ' managers'\n",
      "  Token IDs - Input: 11663, Predicted: 11663\n"
     ]
    }
   ],
   "source": [
    "# Add this to your current debugging cell in roformer_training.ipynb\n",
    "if True:\n",
    "    # Existing initialization checks\n",
    "    print(\"Checking model initialization:\")\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Track intermediate values through the model\n",
    "    with torch.no_grad():\n",
    "        # 1. Check embeddings output\n",
    "        print(\"\\n=== Embeddings Layer ===\")\n",
    "        embedded = model.backbone.embeddings(sample_input_ids)\n",
    "        print(f\"Embeddings output mean: {embedded.mean().item():.6f}\")\n",
    "        print(f\"Embeddings output std: {embedded.std().item():.6f}\")\n",
    "        \n",
    "        # 2. Track through each transformer layer\n",
    "        x = embedded\n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "            print(f\"\\n=== Transformer Layer {i} ===\")\n",
    "            \n",
    "            # 2.1 Self-attention\n",
    "            # Store original input for residual\n",
    "            layer_input = x\n",
    "            \n",
    "            # Get attention outputs\n",
    "            attn_output = layer.self_attn(\n",
    "                q=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                k=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                v=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Attention scores stats:\")\n",
    "            print(f\"  Mean: {attn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {attn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.2 First residual + layer norm\n",
    "            x = layer_input + layer.dropout1(attn_output)\n",
    "            x = layer.ln1(x)\n",
    "            print(f\"After first layer norm:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # 2.3 FFN\n",
    "            ffn_output = layer.ffn(x)\n",
    "            print(f\"FFN output stats:\")\n",
    "            print(f\"  Mean: {ffn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {ffn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.4 Second residual + layer norm\n",
    "            x = x + layer.dropout2(ffn_output)\n",
    "            x = layer.ln2(x)\n",
    "            print(f\"Layer {i} final output:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # Check if output is close to input\n",
    "            similarity = torch.cosine_similarity(layer_input.view(-1), x.view(-1), dim=0)\n",
    "            print(f\"  Cosine similarity with layer input: {similarity.item():.6f}\")\n",
    "        \n",
    "        # 3. Final LM head\n",
    "        print(\"\\n=== LM Head Layer ===\")\n",
    "        logits = model.lm_head(x)\n",
    "        print(f\"Final logits stats:\")\n",
    "        print(f\"  Mean: {logits.mean().item():.6f}\")\n",
    "        print(f\"  Std: {logits.std().item():.6f}\")\n",
    "        \n",
    "        # 4. Check weight tying\n",
    "        print(\"\\n=== Weight Tying Check ===\")\n",
    "        print(f\"Embeddings weight sum: {model.backbone.embeddings.weight.sum().item():.6f}\")\n",
    "        print(f\"LM head weight sum: {model.lm_head.weight.sum().item():.6f}\")\n",
    "        print(f\"Are weights identical? {torch.allclose(model.backbone.embeddings.weight, model.lm_head.weight)}\")\n",
    "        \n",
    "        # 5. Compare predictions with input\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        print(\"\\n=== Input vs Predictions ===\")\n",
    "        print(\"First 5 tokens:\")\n",
    "        for i in range(5):\n",
    "            input_token = tokenizer.decode(sample_input_ids[0, i].item())\n",
    "            pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "            print(f\"Position {i}:\")\n",
    "            print(f\"  Input: '{input_token}'\")\n",
    "            print(f\"  Predicted: '{pred_token}'\")\n",
    "            print(f\"  Token IDs - Input: {sample_input_ids[0, i].item()}, Predicted: {predictions[0, i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer Examples",
   "language": "python",
   "name": "transformer-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
