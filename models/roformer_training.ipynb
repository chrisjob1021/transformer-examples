{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492d7f5e462e4f51baeb1267ca2451ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from utils import TrainingConfig, Config\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")   # or your custom one\n",
    "\n",
    "training_config = TrainingConfig()\n",
    "config = Config(vocab_size=tokenizer.vocab_size,\n",
    "    d_model=768, num_heads=12, ffn_dim=3072,\n",
    "    num_layers=12, max_len=tokenizer.model_max_length )\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load the raw text\n",
    "ds = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = ds.select(range(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b53086fd2c44cd879b40e5f432be37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=training_config.max_len,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.remove_columns([\"attention_mask\"])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def group(batch):\n",
    "#     # Flattens the input_ids and attention_mask into single lists\n",
    "#     flat_ids = sum(batch[\"input_ids\"], [])\n",
    "\n",
    "#     num_of_complete_blocks = len(flat_ids) // config.max_seq_len\n",
    "#     total = num_of_complete_blocks * config.max_seq_len\n",
    "#     flat_ids = flat_ids[:total+1]\n",
    "\n",
    "#     return {\n",
    "#         \"input_ids\": [flat_ids[i:i+config.max_seq_len] for i in range(0, total, config.max_seq_len)],\n",
    "#         \"labels\": [flat_ids[i+1:i+config.max_seq_len+1] for i in range(0, total, config.max_seq_len)]\n",
    "#     }\n",
    "\n",
    "\n",
    "# # lm_ds = tokenized.map(group, batched=True, batch_size=10000)\n",
    "# # lm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RoFormerEncoder, RoFormerForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "model_base = RoFormerEncoder(config)\n",
    "model = RoFormerForCausalLM(model_base, config)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"roformer-base\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8, # Accumulate gradients over N steps\n",
    "    #With gradient accumulation (gradient_accumulation_steps=8):\n",
    "        # You split what would have been one batch into 8 smaller micro-batches\n",
    "        # For each micro-batch, you:\n",
    "        # Load 1/8th of the data into memory\n",
    "        # Do a forward pass (storing 1/8th of the activations)\n",
    "        # Do a backward pass (computing 1/8th of the gradients)\n",
    "        # ACCUMULATE the gradients (don't update weights yet)\n",
    "        # Clear the activations (but keep gradients)\n",
    "    \n",
    "    warmup_steps=10,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_safetensors=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    #With Gradient Checkpointing:\n",
    "        # During the forward pass, only store activations at certain \"checkpoints\"\n",
    "        # During backpropagation, RECOMPUTE the intermediate activations as needed\n",
    "        # This means doing some forward computations twice, but using much less memory\n",
    "    # Without checkpointing, you need to store activations for all 12 layers. With checkpointing, you might only store activations every few layers and recompute the rest during backprop.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=lm_ds,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  31/9375 00:59 < 5:17:04, 0.49 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>425.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>141.448100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:2627\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2626\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2627\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2638\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3211\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3207\u001b[39m         \u001b[38;5;28mself\u001b[39m.state.best_model_checkpoint = best_checkpoint_dir\n\u001b[32m   3209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   3210\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3211\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3212\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_scaler(output_dir)\n\u001b[32m   3213\u001b[39m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3339\u001b[39m, in \u001b[36mTrainer._save_optimizer_and_scheduler\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m   3334\u001b[39m     save_fsdp_optimizer(\n\u001b[32m   3335\u001b[39m         \u001b[38;5;28mself\u001b[39m.accelerator.state.fsdp_plugin, \u001b[38;5;28mself\u001b[39m.accelerator, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.model, output_dir\n\u001b[32m   3336\u001b[39m     )\n\u001b[32m   3337\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m   3338\u001b[39m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3339\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3341\u001b[39m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[32m   3342\u001b[39m is_deepspeed_custom_scheduler = \u001b[38;5;28mself\u001b[39m.is_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   3343\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[32m   3344\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/serialization.py:1216\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1214\u001b[39m     storage = storage.cpu()\n\u001b[32m   1215\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 374])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "sample_batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding weight mean: 0.000124\n",
      "Embedding weight std: 1.000025\n",
      "Input IDs sample: tensor([  40,  447,  247,   76,  407, 3375,  546,  406, 4720,   82],\n",
      "       device='cuda:0')\n",
      "Decoded input: I’m not talking about LEOs\n",
      "Labels sample: tensor([  40,  447,  247,   76,  407, 3375,  546,  406, 4720,   82],\n",
      "       device='cuda:0')\n",
      "Decoded labels: I’m not talking about LEOs\n",
      "\n",
      "Loss: 483.990692\n",
      "\n",
      "Predicted classes sample: tensor([  40,  447,  247,   76,  407, 3375,  546,  406, 4720,   82],\n",
      "       device='cuda:0')\n",
      "Comparison - Predictions vs Labels:\n",
      "Position 0: Predicted 'I' | Label '�'\n",
      "Position 1: Predicted '�' | Label '�'\n",
      "Position 2: Predicted '�' | Label 'm'\n",
      "Position 3: Predicted 'm' | Label ' not'\n",
      "Position 4: Predicted ' not' | Label ' talking'\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # After creating the model but before training\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Print input_ids\n",
    "    print(f\"Input IDs sample: {sample_input_ids[0, :10]}\")  # Print first 10 input IDs of first batch\n",
    "    print(f\"Decoded input: {tokenizer.decode(sample_input_ids[0, :10])}\")  # Decode the first 10 tokens\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(sample_input_ids, labels=sample_labels)\n",
    "    # print(f\"\\nSample batch statistics:\")\n",
    "    # print(f\"Input shape: {sample_input_ids.shape}\")\n",
    "    # print(f\"Labels shape: {sample_labels.shape}\")\n",
    "    print(f\"Labels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\n",
    "    print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\n",
    "\n",
    "    print(f\"\\nLoss: {outputs['loss'].item():.6f}\")\n",
    "\n",
    "    # print(f\"\\nLogits shape: {outputs['logits'].shape}\")\n",
    "    # # Print shapes of intermediate outputs\n",
    "    sequence_length = sample_input_ids.size(1)\n",
    "    # vocab_size = outputs['logits'].shape[-1]\n",
    "    # print(f\"Flattened logits shape: {outputs['logits'].view(batch_size * sequence_length, vocab_size).shape}\")\n",
    "    # print(f\"Flattened labels shape: {sample_labels.view(batch_size * sequence_length).shape}\")\n",
    "\n",
    "    # print(f\"Logits mean: {outputs['logits'].mean().item():.6f}\")\n",
    "    # print(f\"Logits std: {outputs['logits'].std().item():.6f}\")\n",
    "    # print(f\"Logits sample: {outputs['logits'][0, 0, :5]}\")  # Print first 5 logits of first token\n",
    "    \n",
    "    # Get predicted classes from logits\n",
    "    predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "    print(f\"\\nPredicted classes sample: {predictions[0, :10]}\")  # Print first 10 predicted tokens\n",
    "    print(f\"Comparison - Predictions vs Labels:\")\n",
    "    for i in range(min(5, sequence_length)):\n",
    "        pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "        label_token = tokenizer.decode(sample_labels[0, i+1].item()) if sample_labels[0, i+1].item() != -100 else \"[MASKED]\"\n",
    "        print(f\"Position {i}: Predicted '{pred_token}' | Label '{label_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model initialization:\n",
      "Embedding weight mean: -0.000035\n",
      "Embedding weight std: 0.999916\n",
      "\n",
      "=== Embeddings Layer ===\n",
      "Embeddings output mean: -0.001582\n",
      "Embeddings output std: 0.998326\n",
      "\n",
      "=== Transformer Layer 0 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.003008\n",
      "  Std: 0.066774\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.012456\n",
      "  Std: 0.233869\n",
      "Layer 0 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cosine similarity with layer input: 0.967763\n",
      "\n",
      "=== Transformer Layer 1 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.000862\n",
      "  Std: 0.080982\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.012353\n",
      "  Std: 0.236669\n",
      "Layer 1 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.967129\n",
      "\n",
      "=== Transformer Layer 2 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.000418\n",
      "  Std: 0.097013\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.001413\n",
      "  Std: 0.234197\n",
      "Layer 2 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.965975\n",
      "\n",
      "=== Transformer Layer 3 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.002001\n",
      "  Std: 0.106930\n",
      "After first layer norm:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.001594\n",
      "  Std: 0.236986\n",
      "Layer 3 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.965236\n",
      "\n",
      "=== Transformer Layer 4 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.004253\n",
      "  Std: 0.115905\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.001687\n",
      "  Std: 0.232923\n",
      "Layer 4 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.964138\n",
      "\n",
      "=== Transformer Layer 5 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.008181\n",
      "  Std: 0.126545\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.000831\n",
      "  Std: 0.237432\n",
      "Layer 5 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.961067\n",
      "\n",
      "=== Transformer Layer 6 ===\n",
      "Attention scores stats:\n",
      "  Mean: -0.002946\n",
      "  Std: 0.138081\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.007111\n",
      "  Std: 0.234475\n",
      "Layer 6 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.961118\n",
      "\n",
      "=== Transformer Layer 7 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.007337\n",
      "  Std: 0.152214\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.006803\n",
      "  Std: 0.238030\n",
      "Layer 7 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.958343\n",
      "\n",
      "=== Transformer Layer 8 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.003132\n",
      "  Std: 0.161186\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.009883\n",
      "  Std: 0.236669\n",
      "Layer 8 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.957122\n",
      "\n",
      "=== Transformer Layer 9 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.005780\n",
      "  Std: 0.181597\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.004612\n",
      "  Std: 0.236220\n",
      "Layer 9 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.953346\n",
      "\n",
      "=== Transformer Layer 10 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.001344\n",
      "  Std: 0.178087\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: -0.000075\n",
      "  Std: 0.231150\n",
      "Layer 10 final output:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.954064\n",
      "\n",
      "=== Transformer Layer 11 ===\n",
      "Attention scores stats:\n",
      "  Mean: 0.002484\n",
      "  Std: 0.197431\n",
      "After first layer norm:\n",
      "  Mean: -0.000000\n",
      "  Std: 0.999995\n",
      "FFN output stats:\n",
      "  Mean: 0.003498\n",
      "  Std: 0.228389\n",
      "Layer 11 final output:\n",
      "  Mean: 0.000000\n",
      "  Std: 0.999995\n",
      "  Cosine similarity with layer input: 0.951855\n",
      "\n",
      "=== LM Head Layer ===\n",
      "Final logits stats:\n",
      "  Mean: 0.149523\n",
      "  Std: 27.796478\n",
      "\n",
      "=== Weight Tying Check ===\n",
      "Embeddings weight sum: -1343.704224\n",
      "LM head weight sum: -1343.704224\n",
      "Are weights identical? True\n",
      "\n",
      "=== Input vs Predictions ===\n",
      "First 5 tokens:\n",
      "Position 0:\n",
      "  Input: ' the'\n",
      "  Predicted: ' the'\n",
      "  Token IDs - Input: 262, Predicted: 262\n",
      "Position 1:\n",
      "  Input: ' class'\n",
      "  Predicted: ' class'\n",
      "  Token IDs - Input: 1398, Predicted: 1398\n",
      "Position 2:\n",
      "  Input: ' of'\n",
      "  Predicted: ' of'\n",
      "  Token IDs - Input: 286, Predicted: 286\n",
      "Position 3:\n",
      "  Input: ' academic'\n",
      "  Predicted: ' academic'\n",
      "  Token IDs - Input: 8233, Predicted: 8233\n",
      "Position 4:\n",
      "  Input: ' managers'\n",
      "  Predicted: ' managers'\n",
      "  Token IDs - Input: 11663, Predicted: 11663\n"
     ]
    }
   ],
   "source": [
    "# Add this to your current debugging cell in roformer_training.ipynb\n",
    "if True:\n",
    "    # Existing initialization checks\n",
    "    print(\"Checking model initialization:\")\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Track intermediate values through the model\n",
    "    with torch.no_grad():\n",
    "        # 1. Check embeddings output\n",
    "        print(\"\\n=== Embeddings Layer ===\")\n",
    "        embedded = model.backbone.embeddings(sample_input_ids)\n",
    "        print(f\"Embeddings output mean: {embedded.mean().item():.6f}\")\n",
    "        print(f\"Embeddings output std: {embedded.std().item():.6f}\")\n",
    "        \n",
    "        # 2. Track through each transformer layer\n",
    "        x = embedded\n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "            print(f\"\\n=== Transformer Layer {i} ===\")\n",
    "            \n",
    "            # 2.1 Self-attention\n",
    "            # Store original input for residual\n",
    "            layer_input = x\n",
    "            \n",
    "            # Get attention outputs\n",
    "            attn_output = layer.self_attn(\n",
    "                q=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                k=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                v=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Attention scores stats:\")\n",
    "            print(f\"  Mean: {attn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {attn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.2 First residual + layer norm\n",
    "            x = layer_input + layer.dropout1(attn_output)\n",
    "            x = layer.ln1(x)\n",
    "            print(f\"After first layer norm:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # 2.3 FFN\n",
    "            ffn_output = layer.ffn(x)\n",
    "            print(f\"FFN output stats:\")\n",
    "            print(f\"  Mean: {ffn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {ffn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.4 Second residual + layer norm\n",
    "            x = x + layer.dropout2(ffn_output)\n",
    "            x = layer.ln2(x)\n",
    "            print(f\"Layer {i} final output:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # Check if output is close to input\n",
    "            similarity = torch.cosine_similarity(layer_input.view(-1), x.view(-1), dim=0)\n",
    "            print(f\"  Cosine similarity with layer input: {similarity.item():.6f}\")\n",
    "        \n",
    "        # 3. Final LM head\n",
    "        print(\"\\n=== LM Head Layer ===\")\n",
    "        logits = model.lm_head(x)\n",
    "        print(f\"Final logits stats:\")\n",
    "        print(f\"  Mean: {logits.mean().item():.6f}\")\n",
    "        print(f\"  Std: {logits.std().item():.6f}\")\n",
    "        \n",
    "        # 4. Check weight tying\n",
    "        print(\"\\n=== Weight Tying Check ===\")\n",
    "        print(f\"Embeddings weight sum: {model.backbone.embeddings.weight.sum().item():.6f}\")\n",
    "        print(f\"LM head weight sum: {model.lm_head.weight.sum().item():.6f}\")\n",
    "        print(f\"Are weights identical? {torch.allclose(model.backbone.embeddings.weight, model.lm_head.weight)}\")\n",
    "        \n",
    "        # 5. Compare predictions with input\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        print(\"\\n=== Input vs Predictions ===\")\n",
    "        print(\"First 5 tokens:\")\n",
    "        for i in range(5):\n",
    "            input_token = tokenizer.decode(sample_input_ids[0, i].item())\n",
    "            pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "            print(f\"Position {i}:\")\n",
    "            print(f\"  Input: '{input_token}'\")\n",
    "            print(f\"  Predicted: '{pred_token}'\")\n",
    "            print(f\"  Token IDs - Input: {sample_input_ids[0, i].item()}, Predicted: {predictions[0, i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer Examples",
   "language": "python",
   "name": "transformer-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
