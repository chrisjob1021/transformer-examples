{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugpy is listening on localhost:5678\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    import debugpy\n",
    "    \n",
    "    # Try to listen on the port, catch exception if already listening\n",
    "    try:\n",
    "        debugpy.listen((\"localhost\", 5678))\n",
    "        print(\"Debugpy is listening on localhost:5678\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Debugpy is already listening: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from utils import TrainingConfig, Config\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # or your custom one\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "training_config = TrainingConfig()\n",
    "config = Config(vocab_size=tokenizer.vocab_size,\n",
    "d_model=768, num_heads=12, ffn_dim=3072,\n",
    "num_layers=12, max_seq_len=tokenizer.model_max_length )\n",
    "\n",
    "savepath = \"/home/chrisobrien/dev/transformer-examples/models/roformer-base\"\n",
    "\n",
    "if False:\n",
    "    # Create a config dictionary for the model\n",
    "    config_dict = {k: getattr(config, k) for k in vars(config) \n",
    "                if not k.startswith('_') and not callable(getattr(config, k))}\n",
    "\n",
    "    # Save the config as JSON\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "    with open(os.path.join(savepath, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # 1. Load the raw text\n",
    "    ds = load_dataset(\"openwebtext\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "    if False:\n",
    "        ds = ds.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    tokenized = load_from_disk(\"gpt2_tokenized_openwebtext\")\n",
    "    tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = tokenized\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=False,\n",
    "            # max_length=config.max_seq_len,\n",
    "            padding=False,\n",
    "            # return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "    tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"], num_proc=24)\n",
    "    # tokenized = tokenized.remove_columns([\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def group_texts(batch):\n",
    "        concatenated = sum(batch[\"input_ids\"], [])\n",
    "        total_length = len(concatenated)\n",
    "        total_length = (total_length // config.max_seq_len) * config.max_seq_len\n",
    "        result = {\n",
    "            \"input_ids\": [concatenated[i:i+config.max_seq_len] for i in range(0, total_length, config.max_seq_len)]\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    lm_dataset = tokenized.remove_columns([\"attention_mask\"])\n",
    "    if False: lm_dataset = lm_dataset.select(range(1000))\n",
    "    lm_dataset = lm_dataset.map(group_texts, batched=True, num_proc=24)\n",
    "    lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"gpt2_tokenized_concatenated_openwebtext\"\n",
    "if False:\n",
    "    # Save the tokenized dataset to disk\n",
    "    lm_dataset.save_to_disk(dataset_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d33c2beb74646d0a173173e454d6e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    lm_dataset = load_from_disk(dataset_name)\n",
    "    lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 8728153\n",
      "Evaluation dataset size: 88164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 8728153\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 88164\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False: lm_dataset = lm_dataset.select(range(1000))\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_test_split = lm_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Display the datasets\n",
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 1:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 2:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 3:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 4:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 5:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 6:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 7:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 8:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 9:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 10:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 11:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 12:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 13:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 14:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 15:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 16:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 17:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 18:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 19:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 20:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 21:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 22:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 23:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 24:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 25:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 26:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 27:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 28:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 29:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 30:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 31:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 32:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 33:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 34:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 35:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 36:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 37:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 38:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 39:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 40:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 41:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 42:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 43:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 44:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 45:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 46:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 47:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 48:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 49:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 50:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 51:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 52:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 53:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 54:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 55:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 56:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 57:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 58:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 59:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 60:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 61:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 62:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 63:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 64:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 65:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 66:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 67:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 68:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 69:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 70:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 71:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 72:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 73:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 74:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 75:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 76:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 77:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 78:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 79:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 80:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 81:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 82:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 83:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 84:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 85:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 86:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 87:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 88:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 89:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 90:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 91:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 92:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 93:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 94:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 95:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 96:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 97:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 98:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n",
      "\n",
      "Example 99:\n",
      "Sequence length: 1024\n",
      "No padding tokens found\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    ds = lm_dataset\n",
    "    # Let's check a few examples for padding tokens and EOS tokens\n",
    "    for i in range(100):  # Check first 5 examples\n",
    "        # Convert to tensor first, then do the comparison\n",
    "        input_ids = torch.tensor(ds[i][\"input_ids\"])\n",
    "        pad_mask = (input_ids == tokenizer.pad_token_id)\n",
    "        pad_count = pad_mask.sum().item()\n",
    "        \n",
    "        # Check for EOS tokens\n",
    "        eos_mask = (input_ids == tokenizer.eos_token_id)\n",
    "        eos_count = eos_mask.sum().item()\n",
    "        \n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Sequence length: {len(input_ids)}\")\n",
    "        \n",
    "        # Check padding tokens\n",
    "        if pad_count > 0:\n",
    "            print(f\"Pad tokens found: {pad_count}\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"No padding tokens found\")\n",
    "        \n",
    "        # # Check EOS tokens\n",
    "        # if eos_count > 0:\n",
    "        #     print(f\"EOS tokens found: {eos_count}\")\n",
    "        #     # Show where the EOS tokens are\n",
    "        #     eos_positions = (input_ids == tokenizer.eos_token_id).nonzero().flatten().tolist()\n",
    "        #     print(f\"EOS token positions: {eos_positions}\")\n",
    "        # else:\n",
    "        #     print(\"No EOS tokens found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load the tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the tokenized dataset from disk\n",
    "    from datasets import load_from_disk\n",
    "\n",
    "    dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "    try:\n",
    "        print(\"Loading tokenized dataset from disk...\")\n",
    "        tokenized = load_from_disk(dataset_name)\n",
    "        print(f\"Successfully loaded tokenized dataset with {len(tokenized)} examples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dataset not found at {dataset_name}. Please make sure you've saved the tokenized dataset first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Save the tokenized dataset to disk\n",
    "    dataset_name = \"gpt2_tokenized_openwebtext\"\n",
    "    tokenized.save_to_disk(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Upload to Hugging Face Hub\n",
    "    # You'll need to be logged in to Hugging Face\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # Login to Hugging Face (you'll need to run this once and enter your token)\n",
    "    # Uncomment the line below when you're ready to login\n",
    "    login(\"hf_xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Save the tokenized dataset to disk\n",
    "    username = \"chrisjob1021\"\n",
    "\n",
    "    # Upload to Hugging Face Hub\n",
    "    # You'll need to be logged in to Hugging Face\n",
    "    from huggingface_hub import HfApi\n",
    "\n",
    "    # Initialize the Hugging Face API\n",
    "    api = HfApi()\n",
    "\n",
    "    # Upload the dataset to the Hub\n",
    "    # Replace \"your-username/tokenized-openwebtext\" with your desired repository name\n",
    "    try:\n",
    "        api.create_repo(\n",
    "            repo_id=username + \"/\" + dataset_name,\n",
    "            repo_type=\"dataset\",\n",
    "            exist_ok=True\n",
    "        )\n",
    "        \n",
    "        api.upload_folder(\n",
    "            folder_path=dataset_name,\n",
    "            repo_id=username + \"/\" + dataset_name,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        \n",
    "        print(\"Dataset successfully uploaded to Hugging Face Hub!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading dataset: {e}\")\n",
    "        print(\"You may need to login first with `login()` or check your permissions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoint-9500\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from models import RoFormerForCausalLM, RoFormerDecoder\n",
    "import torch\n",
    "\n",
    "if False:\n",
    "    # model initialization\n",
    "    model_base = RoFormerDecoder(config)\n",
    "    model = RoFormerForCausalLM(model_base, config)\n",
    "if True: model = RoFormerForCausalLM.from_pretrained(savepath)\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "import os\n",
    "# Get the absolute path for logs\n",
    "log_dir = os.path.join(os.path.dirname(savepath), \"logs\")\n",
    "# Create the logging directory if it doesn't exist\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=savepath,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=8, # Accumulate gradients over N steps\n",
    "    #With gradient accumulation (gradient_accumulation_steps=8):\n",
    "        # You split what would have been one batch into 8 smaller micro-batches\n",
    "        # For each micro-batch, you:\n",
    "        # Load 1/8th of the data into memory\n",
    "        # Do a forward pass (storing 1/8th of the activations)\n",
    "        # Do a backward pass (computing 1/8th of the gradients)\n",
    "        # ACCUMULATE the gradients (don't update weights yet)\n",
    "        # Clear the activations (but keep gradients)\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    warmup_steps=100,\n",
    "    logging_dir=log_dir,\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_safetensors=False,\n",
    "    # report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    #With Gradient Checkpointing:\n",
    "        # During the forward pass, only store activations at certain \"checkpoints\"\n",
    "        # During backpropagation, RECOMPUTE the intermediate activations as needed\n",
    "        # This means doing some forward computations twice, but using much less memory\n",
    "    # Without checkpointing, you need to store activations for all 12 layers. With checkpointing, you might only store activations every few layers and recompute the rest during backprop.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/70 00:58 < 06:21, 0.15 it/s, Epoch 1.27/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>411.670500</td>\n",
       "      <td>435.620026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     trainer.train(resume_from_checkpoint=savepath + \u001b[33m\"\u001b[39m\u001b[33m/checkpoint-210\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/transformer-examples/models/roformer.py:162\u001b[39m, in \u001b[36mRoFormerForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels.device.type == \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLabels tensor is on CPU but should be on GPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden.device.type == \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mHidden state tensor is on CPU but should be on GPU\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/transformer-examples/models/roformer.py:76\u001b[39m, in \u001b[36mRoFormerDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     73\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/transformer-examples/models/roformer.py:40\u001b[39m, in \u001b[36mRoFormerDecoderLayer.forward\u001b[39m\u001b[34m(self, x, attention_mask)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput tensor \u001b[39m\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is on CPU but should be on GPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# batch_size, seq_len = x.shape[0], x.shape[1]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.config.d_model)\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn_output.device.type == \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/transformer-examples/attention/multi_head_attention.py:90\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, attention_mask)\u001b[39m\n\u001b[32m     88\u001b[39m causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=\u001b[32m1\u001b[39m).bool()\n\u001b[32m     89\u001b[39m causal_mask = causal_mask.unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# [1, 1, seq_len, seq_len]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m causal_mask = \u001b[43mcausal_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move to same device as qk\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Use causal mask\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# When this mask is used with masked_fill(causal_mask, float(\"-inf\")), \u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# it sets all True values to negative infinity. \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# and previous tokens, which is crucial for tasks like language modeling \u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# where you want to prevent the model from \"seeing into the future\" during training and inference.\u001b[39;00m\n\u001b[32m    103\u001b[39m qk = qk.masked_fill(causal_mask, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    trainer.train(resume_from_checkpoint=savepath + \"/checkpoint-210\")\n",
    "if False:\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  632,   447,   247,  ...,   423,   890, 24067],\n",
       "        [  447,   247,    83,  ...,  4585,   319,   584],\n",
       "        [  319,   884,   355,  ...,  1276,   307,  3177],\n",
       "        [ 1238,  2431,  1804,  ...,  1842,   351,  7906]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[  632,   447,   247,  ...,   423,   890, 24067],\n",
       "        [  447,   247,    83,  ...,  4585,   319,   584],\n",
       "        [  319,   884,   355,  ...,  1276,   307,  3177],\n",
       "        [ 1238,  2431,  1804,  ...,  1842,   351,  7906]], device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(438.8967, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'logits': tensor([[[-1.1708e+01,  5.5505e+01, -1.0244e+01,  ...,  1.3652e+01,\n",
       "            2.7981e+01, -3.2116e+01],\n",
       "          [-2.1562e+00,  4.6054e+00, -1.4834e+01,  ...,  1.0042e+01,\n",
       "            7.1052e+00, -2.9168e+01],\n",
       "          [-3.4085e+00, -3.4882e+00,  1.5006e+01,  ...,  1.3831e+01,\n",
       "            1.5407e+00, -5.5329e+00],\n",
       "          ...,\n",
       "          [ 4.5676e+01,  7.5806e+01,  2.0260e+01,  ...,  1.5575e+01,\n",
       "            4.3080e+01, -4.1471e+00],\n",
       "          [ 4.7887e+01,  4.3107e+00,  4.2337e+01,  ...,  7.1704e+00,\n",
       "            2.9221e+01, -1.9452e+01],\n",
       "          [ 4.8769e+01,  6.8928e+01, -2.9382e-01,  ...,  6.2208e+00,\n",
       "            1.0754e+01, -3.2028e+01]],\n",
       " \n",
       "         [[-3.5484e+01, -3.9089e+01,  5.3055e+01,  ...,  1.4895e+00,\n",
       "           -1.0835e+01,  9.0755e+00],\n",
       "          [-2.7777e+01, -2.3661e+01,  6.9394e+01,  ..., -2.5212e+01,\n",
       "           -1.4575e+01, -1.8849e+01],\n",
       "          [-4.1318e+01, -1.5594e+01,  5.9812e+01,  ..., -4.0312e+01,\n",
       "            2.2224e+01, -5.9817e+01],\n",
       "          ...,\n",
       "          [-1.6911e+01,  2.9742e+01,  1.8841e+01,  ...,  2.0924e+01,\n",
       "            3.7863e+01, -1.9224e+01],\n",
       "          [-6.1570e+00,  3.7826e+00, -8.2856e+00,  ...,  1.5015e+01,\n",
       "            3.9539e+01,  7.7938e+00],\n",
       "          [-5.0420e+00,  4.2770e+01,  1.5858e+01,  ..., -9.5417e+00,\n",
       "            4.5824e-01, -1.3383e+01]],\n",
       " \n",
       "         [[ 4.3642e+01,  4.9994e+01,  6.0095e+00,  ...,  1.8132e+01,\n",
       "            4.4573e+01,  4.1276e+00],\n",
       "          [-7.7916e+00,  8.3052e+01,  1.3444e+01,  ..., -1.5407e+01,\n",
       "            4.1047e+01, -6.8122e+00],\n",
       "          [ 1.8010e+01,  3.3849e+01,  1.0138e+01,  ...,  2.7660e+00,\n",
       "            2.2615e+00,  1.4269e+01],\n",
       "          ...,\n",
       "          [ 1.8488e-01,  1.0024e+01,  1.1273e+01,  ..., -1.1165e+01,\n",
       "            9.7442e+00, -5.1727e+01],\n",
       "          [-3.2631e+01,  5.3825e+01, -6.1129e+00,  ...,  1.1393e+01,\n",
       "           -9.9827e+00, -2.0276e+01],\n",
       "          [-1.8652e+00, -7.9217e+00,  2.4248e+01,  ..., -1.1121e+01,\n",
       "            5.7050e+00,  1.3535e+01]],\n",
       " \n",
       "         [[-2.2715e+01,  2.1227e+01,  5.6677e+00,  ..., -5.4588e+01,\n",
       "            5.3054e+01, -4.3382e+01],\n",
       "          [-2.2280e+01,  2.4078e+01, -3.7590e+01,  ..., -6.3903e+00,\n",
       "            5.4663e+01, -1.2506e+01],\n",
       "          [ 7.5207e+00,  3.5454e+01, -1.5284e+01,  ..., -2.7037e+01,\n",
       "            5.8054e+01, -2.5864e+01],\n",
       "          ...,\n",
       "          [-8.6506e+00,  2.9117e+01,  2.2753e+01,  ..., -1.9213e+00,\n",
       "            2.1411e+01, -1.5627e+01],\n",
       "          [ 5.4547e+01,  1.9810e+01,  3.9409e+01,  ...,  4.0150e+00,\n",
       "            3.8478e-02,  5.3166e+00],\n",
       "          [-2.1076e+01,  1.6972e+01,  6.2505e+00,  ..., -2.3454e+01,\n",
       "           -5.9252e-01,  3.9744e+01]]], device='cuda:0',\n",
       "        grad_fn=<UnsafeViewBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_batch['input_ids'], sample_batch['attention_mask'], sample_batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Clear CUDA cache to free up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print memory stats before and after clearing cache\n",
    "    print(f\"GPU memory allocated before clearing cache: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory allocated after clearing cache: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Optional: force garbage collection as well\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded input:  revealed to be the largest climatic and\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 44.39 GiB of which 1.63 GiB is free. Including non-PyTorch memory, this process has 42.75 GiB memory in use. Of the allocated memory 41.88 GiB is allocated by PyTorch, and 388.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDecoded input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer.decode(sample_input_ids[\u001b[32m0\u001b[39m,\u001b[38;5;250m \u001b[39m:\u001b[32m10\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Decode the first 10 tokens\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# print(f\"\\nSample batch statistics:\")\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# print(f\"Input shape: {sample_input_ids.shape}\")\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# print(f\"Labels shape: {sample_labels.shape}\")\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(f\"Labels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m].item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/venvs/transformer-examples/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/transformer-examples/models/roformer.py:174\u001b[39m, in \u001b[36mRoFormerForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels)\u001b[39m\n\u001b[32m    170\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# Flatten the logits and labels for cross entropy loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     shift_logits = \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     shift_labels = labels[:, \u001b[32m1\u001b[39m:].contiguous()\n\u001b[32m    176\u001b[39m     loss = F.cross_entropy(\n\u001b[32m    177\u001b[39m             shift_logits.view(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)),\n\u001b[32m    178\u001b[39m             shift_labels.view(-\u001b[32m1\u001b[39m),\n\u001b[32m    179\u001b[39m             ignore_index=-\u001b[32m100\u001b[39m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 44.39 GiB of which 1.63 GiB is free. Including non-PyTorch memory, this process has 42.75 GiB memory in use. Of the allocated memory 41.88 GiB is allocated by PyTorch, and 388.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # # After creating the model but before training\n",
    "    # print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    # print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Print input_ids\n",
    "    # print(f\"Input IDs sample: {sample_input_ids[0, :10]}\")  # Print first 10 input IDs of first batch\n",
    "    print(f\"Decoded input: {tokenizer.decode(sample_input_ids[0, :10])}\")  # Decode the first 10 tokens\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(sample_input_ids, labels=sample_labels)\n",
    "    # print(f\"\\nSample batch statistics:\")\n",
    "    # print(f\"Input shape: {sample_input_ids.shape}\")\n",
    "    # print(f\"Labels shape: {sample_labels.shape}\")\n",
    "    # print(f\"Labels sample: {sample_labels[0, :10]}\")  # Print first 10 labels of first batch\n",
    "    # print(f\"Decoded labels: {tokenizer.decode([l.item() for l in sample_labels[0, :10] if l.item() != -100])}\")  # Decode the first 10 labels, skipping masked tokens\n",
    "\n",
    "    print(f\"Loss: {outputs['loss'].item():.6f}\")\n",
    "\n",
    "    # print(f\"\\nLogits shape: {outputs['logits'].shape}\")\n",
    "    # # Print shapes of intermediate outputs\n",
    "    sequence_length = sample_input_ids.size(1)\n",
    "    # vocab_size = outputs['logits'].shape[-1]\n",
    "    # print(f\"Flattened logits shape: {outputs['logits'].view(batch_size * sequence_length, vocab_size).shape}\")\n",
    "    # print(f\"Flattened labels shape: {sample_labels.view(batch_size * sequence_length).shape}\")\n",
    "\n",
    "    # print(f\"Logits mean: {outputs['logits'].mean().item():.6f}\")\n",
    "    # print(f\"Logits std: {outputs['logits'].std().item():.6f}\")\n",
    "    # print(f\"Logits sample: {outputs['logits'][0, 0, :5]}\")  # Print first 5 logits of first token\n",
    "    \n",
    "    # print(f\"\\nTop 3 predicted tokens sample: {topk_indices[0, :10]}\")  # Print first 10 sets of predictions\n",
    "    \n",
    "    # Add top-p (nucleus) sampling\n",
    "    logits = outputs['logits'][0, :10]  # First batch, first 10 positions\n",
    "    top_k = 50\n",
    "    topk_values, topk_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "    softmax_logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    print(f\"\\nComparison - Top-k and Top-p Predictions vs Labels:\")\n",
    "    for i in range(min(5, sequence_length)):\n",
    "        # Top-k results\n",
    "        top_k_tokens = []\n",
    "        for k in range(5):\n",
    "            token = tokenizer.decode(topk_indices[i, k].item())\n",
    "            top_k_tokens.append(f\"{k+1}.'{token}'\")\n",
    "        top_k_str = \" \".join(top_k_tokens)\n",
    "        \n",
    "        # Top-p (nucleus) sampling\n",
    "        sorted_probs, sorted_indices = torch.sort(softmax_logits[i], descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        nucleus_indices = sorted_indices[cumulative_probs <= 0.9]\n",
    "        nucleus_size = len(nucleus_indices)\n",
    "        \n",
    "        # Sample from the nucleus\n",
    "        if nucleus_size > 0:\n",
    "            nucleus_probs = sorted_probs[:nucleus_size]\n",
    "            nucleus_probs = nucleus_probs / nucleus_probs.sum()  # Renormalize probabilities\n",
    "            nucleus_sample_idx = torch.multinomial(nucleus_probs, 1).item()\n",
    "            nucleus_token_idx = nucleus_indices[nucleus_sample_idx].item()\n",
    "            nucleus_token = tokenizer.decode(nucleus_token_idx)\n",
    "        else:\n",
    "            nucleus_token = tokenizer.decode(sorted_indices[0].item())\n",
    "        # Get actual label\n",
    "        label_token = tokenizer.decode(sample_labels[0, i+1].item()) if sample_labels[0, i+1].item() != -100 else \"[MASKED]\"\n",
    "        # Print results\n",
    "        print(f\"Position {i}:\")\n",
    "        # print(f\"  Top-k: 1.'{top1_token}' 2.'{top2_token}' 3.'{top3_token}'\")\n",
    "        print(f\"  Top-k: {top_k_str}\")\n",
    "        print(f\"  Top-p: nucleus size={nucleus_size} (p=0.9), sampled='{nucleus_token}'\")\n",
    "        print(f\"  Label: '{label_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your current debugging cell in roformer_training.ipynb\n",
    "if False:\n",
    "    # Existing initialization checks\n",
    "    print(\"Checking model initialization:\")\n",
    "    print(f\"Embedding weight mean: {model.backbone.embeddings.weight.mean().item():.6f}\")\n",
    "    print(f\"Embedding weight std: {model.backbone.embeddings.weight.std().item():.6f}\")\n",
    "\n",
    "    # Sample a small batch\n",
    "    sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "    sample_input_ids = sample_batch['input_ids'].to(device)\n",
    "    sample_labels = sample_batch['labels'].to(device)\n",
    "    \n",
    "    # Track intermediate values through the model\n",
    "    with torch.no_grad():\n",
    "        # 1. Check embeddings output\n",
    "        print(\"\\n=== Embeddings Layer ===\")\n",
    "        embedded = model.backbone.embeddings(sample_input_ids)\n",
    "        print(f\"Embeddings output mean: {embedded.mean().item():.6f}\")\n",
    "        print(f\"Embeddings output std: {embedded.std().item():.6f}\")\n",
    "        \n",
    "        # 2. Track through each transformer layer\n",
    "        x = embedded\n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "            print(f\"\\n=== Transformer Layer {i} ===\")\n",
    "            \n",
    "            # 2.1 Self-attention\n",
    "            # Store original input for residual\n",
    "            layer_input = x\n",
    "            \n",
    "            # Get attention outputs\n",
    "            attn_output = layer.self_attn(\n",
    "                q=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                k=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2),\n",
    "                v=x.view(x.size(0), x.size(1), layer.config.num_heads, layer.config.per_head_dim).transpose(1, 2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Attention scores stats:\")\n",
    "            print(f\"  Mean: {attn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {attn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.2 First residual + layer norm\n",
    "            x = layer_input + layer.dropout1(attn_output)\n",
    "            x = layer.ln1(x)\n",
    "            print(f\"After first layer norm:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # 2.3 FFN\n",
    "            ffn_output = layer.ffn(x)\n",
    "            print(f\"FFN output stats:\")\n",
    "            print(f\"  Mean: {ffn_output.mean().item():.6f}\")\n",
    "            print(f\"  Std: {ffn_output.std().item():.6f}\")\n",
    "            \n",
    "            # 2.4 Second residual + layer norm\n",
    "            x = x + layer.dropout2(ffn_output)\n",
    "            x = layer.ln2(x)\n",
    "            print(f\"Layer {i} final output:\")\n",
    "            print(f\"  Mean: {x.mean().item():.6f}\")\n",
    "            print(f\"  Std: {x.std().item():.6f}\")\n",
    "            \n",
    "            # Check if output is close to input\n",
    "            similarity = torch.cosine_similarity(layer_input.view(-1), x.view(-1), dim=0)\n",
    "            print(f\"  Cosine similarity with layer input: {similarity.item():.6f}\")\n",
    "        \n",
    "        # 3. Final LM head\n",
    "        print(\"\\n=== LM Head Layer ===\")\n",
    "        logits = model.lm_head(x)\n",
    "        print(f\"Final logits stats:\")\n",
    "        print(f\"  Mean: {logits.mean().item():.6f}\")\n",
    "        print(f\"  Std: {logits.std().item():.6f}\")\n",
    "        \n",
    "        # 4. Check weight tying\n",
    "        print(\"\\n=== Weight Tying Check ===\")\n",
    "        print(f\"Embeddings weight sum: {model.backbone.embeddings.weight.sum().item():.6f}\")\n",
    "        print(f\"LM head weight sum: {model.lm_head.weight.sum().item():.6f}\")\n",
    "        print(f\"Are weights identical? {torch.allclose(model.backbone.embeddings.weight, model.lm_head.weight)}\")\n",
    "        \n",
    "        # 5. Compare predictions with input\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        print(\"\\n=== Input vs Predictions ===\")\n",
    "        print(\"First 5 tokens:\")\n",
    "        for i in range(5):\n",
    "            input_token = tokenizer.decode(sample_input_ids[0, i].item())\n",
    "            pred_token = tokenizer.decode(predictions[0, i].item())\n",
    "            print(f\"Position {i}:\")\n",
    "            print(f\"  Input: '{input_token}'\")\n",
    "            print(f\"  Predicted: '{pred_token}'\")\n",
    "            print(f\"  Token IDs - Input: {sample_input_ids[0, i].item()}, Predicted: {predictions[0, i].item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer Examples",
   "language": "python",
   "name": "transformer-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
