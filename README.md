# Transformer Examples

This repository contains a collection of toy implementations and examples of key components from modern Transformer architectures. Each example is designed to be educational, well-documented, and easy to understand.

## Components

| Component | Description | Paper |
|-----------|-------------|-------|
| [Multi-Head Latent Attention (MLA)](./attention) | A novel attention mechanism from DeepSeek V2 that uses latent queries to reduce KV cache and Rotary Position Embeddings | [DeepSeek V2 Technical Report](https://arxiv.org/abs/2405.04434) |

## Getting Started

Each component has its own directory with:
- Implementation code
- Jupyter notebook with examples (and visualizations)
- Requirements file for dependencies

To get started with a specific component, navigate to its directory and follow the instructions in its README.

## Contributing

Contributions are welcome! If you'd like to add a new component or improve an existing one, please feel free to submit a pull request.

## License

This project is licensed under the MIT License - see the LICENSE file for details. 