{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autograd  ∂L/∂W\n",
      " tensor([[-0.0538, -0.1118,  0.0960],\n",
      "        [ 0.3203,  0.6655, -0.5712]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# one_neuron_grad.py  –  per-neuron Kronecker view\n",
    "# -----------------------------------------------------------\n",
    "import torch, math\n",
    "torch.manual_seed(0)\n",
    "dtype = torch.float64\n",
    "\n",
    "m, n = 2, 3                     # outputs, inputs  (tiny toy layer)\n",
    "\n",
    "# ----- tiny random tensors -----\n",
    "W = torch.randn(m, n, requires_grad=True)\n",
    "x = torch.randn(n)              # input vector (n,)\n",
    "g = torch.randn(m)              # gain vector\n",
    "b = torch.randn(m)              # bias vector\n",
    "\n",
    "# ----- forward pass -----\n",
    "a = W @ x                       # (m,)\n",
    "u = torch.sqrt((a ** 2).mean()) # RMS scale (scalar)\n",
    "v = g * (a / u) + b             # (m,)\n",
    "loss = v.pow(2).sum()           # simple scalar loss\n",
    "\n",
    "# ----- Autograd reference -----\n",
    "loss.backward()\n",
    "grad_auto = W.grad.detach()   # (m × n)\n",
    "print(\"\\nAutograd  ∂L/∂W\\n\", grad_auto, grad_auto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4033,  0.8380, -0.7193]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(1).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R tensor([[ 0.0197, -0.1173],\n",
      "        [-0.1173,  0.6982]], grad_fn=<MulBackward0>) torch.Size([2, 2])\n",
      "x tensor([ 0.4033,  0.8380, -0.7193]) torch.Size([3])\n",
      "J shape: torch.Size([2, 6])\n",
      "\n",
      "J matrix structure:\n",
      "Row 0: tensor([ 0.4033,  0.8380, -0.7193,  0.0000,  0.0000, -0.0000])\n",
      "Row 1: tensor([ 0.0000,  0.0000, -0.0000,  0.4033,  0.8380, -0.7193])\n",
      "grad_vec tensor([[-0.0538, -0.1118,  0.0960,  0.3203,  0.6655, -0.5712]],\n",
      "       grad_fn=<MmBackward0>) torch.Size([1, 6])\n",
      "grad_an shape: torch.Size([2, 3])\n",
      "grad_an:\n",
      " tensor([[-0.0538, -0.1118,  0.0960],\n",
      "        [ 0.3203,  0.6655, -0.5712]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Analytic ∂L/∂W\n",
      " tensor([[-0.0538, -0.1118,  0.0960],\n",
      "        [ 0.3203,  0.6655, -0.5712]], grad_fn=<ViewBackward0>) torch.Size([2, 3])\n",
      "\n",
      "Autograd  ∂L/∂W\n",
      " tensor([[-0.0538, -0.1118,  0.0960],\n",
      "        [ 0.3203,  0.6655, -0.5712]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# ------------ analytic gradient ----------------------------\n",
    "d = 2 * v.detach()                                  # ∂L/∂v  (because L = Σ v_i²)\n",
    "I = torch.eye(m, dtype=dtype)                       # Identity matrix\n",
    "R = (1/u) * (I - torch.outer(a, a) / (m * u**2))    # (m × m)\n",
    "print(\"R\", R, R.shape)\n",
    "# P = torch.diag(g * d) @ R                           # (m x m) diag(g ⊙ d) · R\n",
    "# print(\"P\", P, P.shape)\n",
    "# print(\"x\", x, x.shape)\n",
    "# Calculate grad_an using the Kronecker product method\n",
    "row      = (d * g) @ R                        # (m,) - this is your P @ (d*g) \n",
    "# The Kronecker product with the identity matrix is implementing the Kronecker delta \n",
    "print(\"x\", x, x.shape)\n",
    "J        = torch.kron(I, x.unsqueeze(0))      # (m, m*n) - Jacobian matrix\n",
    "# I[0,0] = 1  →  1 * [a, b, c] = [a, b, c]\n",
    "# I[0,1] = 0  →  0 * [a, b, c] = [0, 0, 0]\n",
    "# I[1,0] = 0  →  0 * [a, b, c] = [0, 0, 0]\n",
    "# I[1,1] = 1  →  1 * [a, b, c] = [a, b, c]\n",
    "# J = [[a, b, c, 0, 0, 0],\n",
    "#      [0, 0, 0, a, b, c]]\n",
    "# The identity matrix I provides the Kronecker delta structure\n",
    "# The Kronecker product spreads this pattern across the x values.\n",
    "print(\"J shape:\", J.shape)\n",
    "print(\"\\nJ matrix structure:\")\n",
    "for i in range(J.shape[0]):\n",
    "    print(f\"Row {i}:\", J[i])\n",
    "grad_vec = row.unsqueeze(0) @ J               # (1, m*n) - apply chain rule\n",
    "print(\"grad_vec\", grad_vec, grad_vec.shape)\n",
    "grad_an  = grad_vec.view(m, n) \n",
    "\n",
    "print(\"grad_an shape:\", grad_an.shape)\n",
    "print(\"grad_an:\\n\", grad_an)\n",
    "# grad_an   = P @ x.unsqueeze(0).transpose(0, 1)              # outer product with xᵀ  → (m × n)\n",
    "print(\"\\nAnalytic ∂L/∂W\\n\", grad_an, grad_an.shape)\n",
    "print(\"\\nAutograd  ∂L/∂W\\n\", grad_auto, grad_auto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max |Δ|  : 0.0\n"
     ]
    }
   ],
   "source": [
    "# ------------ numeric check --------------------------------\n",
    "print(\"\\nMax |Δ|  :\", (grad_an - grad_auto).abs().max().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
