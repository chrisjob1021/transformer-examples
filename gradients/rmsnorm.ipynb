{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autograd  ∂L/∂W\n",
      " tensor([[-0.0538, -0.1118,  0.0960],\n",
      "        [ 0.3203,  0.6655, -0.5712]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "torch.manual_seed(0)\n",
    "dtype = torch.float64\n",
    "\n",
    "m, n = 2, 3                     # outputs, inputs  (tiny toy layer)\n",
    "\n",
    "# ----- tiny random tensors -----\n",
    "W = torch.randn(m, n, requires_grad=True)   # The weight matrix W has rows that correspond one-to-one to the output neurons.\n",
    "                                            # Row 0 are all the weights feeding neuron 0\n",
    "x = torch.randn(n, requires_grad=True)      # input vector (n,)\n",
    "g = torch.randn(m, requires_grad=True)      # gain vector\n",
    "b = torch.randn(m, requires_grad=True)      # bias vector\n",
    "\n",
    "# ----- forward pass -----\n",
    "a = W @ x                                   # a_i is the pre-activation (the weighted sum before any non-linearity) of neuron i\n",
    "                                            # The activation vector a collects those neurons: a_0, a_1, ..., a_{m-1}\n",
    "                                            # So a_0 is literally the output of \"neuron 0\": a_0 = W0,q @ x_q\n",
    "a.retain_grad()\n",
    "u = torch.sqrt((a ** 2).mean()) # RMS scale (scalar)\n",
    "v = g * (a / u) + b\n",
    "v.retain_grad()\n",
    "loss = v.pow(2).sum()           # simple scalar loss\n",
    "\n",
    "# ----- Autograd reference -----\n",
    "loss.backward()\n",
    "grad_auto = W.grad.detach()   # (m × n)\n",
    "bias_grad = b.grad.detach()\n",
    "print(\"\\nAutograd  ∂L/∂W\\n\", grad_auto, grad_auto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aL_av tensor([[-0.7610, -1.9929]]) torch.Size([1, 2])\n",
      "∂L/∂b gradients match: True\n",
      "∂L/∂g gradients match: True\n",
      "aa_aW tensor([[ 0.4033,  0.8380, -0.7193,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  0.4033,  0.8380, -0.7193]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([2, 6])\n",
      "∂L/∂W gradients match: True\n"
     ]
    }
   ],
   "source": [
    "# ------------ analytic gradient ----------------------------\n",
    "aL_av = 2 * v.detach()                                  # ∂L/∂v  (because L = Σ v_i²)\n",
    "aL_av = aL_av.unsqueeze(0)\n",
    "print(\"aL_av\", aL_av, aL_av.shape)\n",
    "\n",
    "aL_ab = d\n",
    "print(\"∂L/∂b gradients match:\", torch.allclose(aL_ab, b.grad))\n",
    "\n",
    "aL_ag = aL_av * (a / u)\n",
    "print(\"∂L/∂g gradients match:\", torch.allclose(aL_ag, g.grad))\n",
    "\n",
    "I = torch.eye(m) # (m, m)\n",
    "a_aa = 1/u * (I - torch.outer(a, a) / (m * u**2)) # (m, m)\n",
    "R = a_aa # (m, m)\n",
    "R.retain_grad()\n",
    "\n",
    "av_aa = torch.diag(g) @ R # (m, m)\n",
    "\n",
    "# ∂a_i / ∂W_pq = δ_ip * x_q\n",
    "\n",
    "# i is which activation we are taking the derivative of (e.g. a_0)\n",
    "# p is the weight you perturb (i.e. which neuron the weight belongs to) -- row in W\n",
    "# q is the column index of the weight (which input feature it multiplies) -- e.g. x_1\n",
    "\n",
    "# If you perturb a weight in same row p, the change in activation i is \"input * weight change\"\n",
    "# If you perturb a weight in different row p, the change in activation i is 0\n",
    "aa_aW = torch.kron(I, x)\n",
    "print(\"aa_aW\", aa_aW, aa_aW.shape) # (m, m*n)\n",
    "\n",
    "aL_aW = aL_av @ av_aa @ aa_aW\n",
    "aL_aW = aL_aW.view(m, n)\n",
    "print(\"∂L/∂W gradients match:\", torch.allclose(aL_aW, W.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative of Wx with respect to W (col-major)\n",
      "Kronecker Jacobian Iₘ ⊗ xᵀ:\n",
      " tensor([[ 1.3000,  0.0000,  0.9000,  0.0000, -0.7000, -0.0000],\n",
      "        [ 0.0000,  1.3000,  0.0000,  0.9000, -0.0000, -0.7000]]) \n",
      "\n",
      "Col-major structure:\n",
      "[[x1  0  x2   0   x3   0 ]\n",
      " [ 0   x1   0  x2  0  x3]]\n",
      "\n",
      "i=0, p=0, q=0:  J = 1.30  δ·x = 1.30\n",
      "i=0, p=0, q=1:  J = 0.90  δ·x = 0.90\n",
      "i=0, p=1, q=0:  J = 0.00  δ·x = 0.00\n",
      "i=0, p=1, q=1:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=0:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=1:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=1, q=0:  J = 1.30  δ·x = 1.30\n",
      "i=1, p=1, q=1:  J = 0.90  δ·x = 0.90\n"
     ]
    }
   ],
   "source": [
    "# column-major (Fortran / MATLAB default)\n",
    "#“weights are stored column-by-column, so route the column-gradient (xᵀ) first, then scale each entire column by the inputs (Iₘ).\n",
    "J = torch.kron(x.unsqueeze(0), I)\n",
    "\n",
    "print(\"Partial derivative of Wx with respect to W (col-major)\")\n",
    "print(\"Kronecker Jacobian Iₘ ⊗ xᵀ:\\n\", J, \"\\n\")\n",
    "\n",
    "# Show the structure of the row-major Jacobian\n",
    "print(\"Col-major structure:\")\n",
    "print(\"[[x1  0  x2   0   x3   0 ]\")\n",
    "print(\" [ 0   x1   0  x2  0  x3]]\\n\")\n",
    "\n",
    "# Row in W corresponds to a neuron\n",
    "# Column in W corresponds to a weight that will multiply with feature in x (column in x)\n",
    "# Result is the neuron's activation\n",
    "# Ji,j = x_t * I_m = δ_{ip} * x_q\n",
    "\n",
    "# Verify the Jacobian entries\n",
    "for i, p, q in itertools.product(range(m), repeat=3):\n",
    "    j = q*m + p\n",
    "    if q < n:   # keep q in 0..n-1\n",
    "        lhs = J[i, j].item()\n",
    "        rhs = (1.0 if i==p else 0.0)*x[q].item()\n",
    "        print(f\"i={i}, p={p}, q={q}:  J = {lhs:.2f}  δ·x = {rhs:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative of Wx with respect to W (row-major)\n",
      "Kronecker Jacobian Iₘ ⊗ xᵀ:\n",
      " tensor([[ 1.3000,  0.9000, -0.7000,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  1.3000,  0.9000, -0.7000]]) \n",
      "\n",
      "Row-major structure:\n",
      "[[x1  x2  x3   0   0   0 ]\n",
      " [ 0   0   0  x1  x2  x3]]\n",
      "\n",
      "i=0, p=0, q=0:  J = 1.30  δ·x = 1.30\n",
      "i=0, p=0, q=1:  J = 0.90  δ·x = 0.90\n",
      "i=0, p=1, q=0:  J = 0.00  δ·x = 0.00\n",
      "i=0, p=1, q=1:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=0:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=1:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=1, q=0:  J = 1.30  δ·x = 1.30\n",
      "i=1, p=1, q=1:  J = 0.90  δ·x = 0.90\n"
     ]
    }
   ],
   "source": [
    "# row-major (C / PyTorch default)\t\n",
    "# weights are stored row-by-row, so route the row-gradient (Iₘ) first, then scale each entire row by the inputs (xᵀ)\n",
    "J = torch.kron(I, x.unsqueeze(0))  # (m × m n)\n",
    "\n",
    "print(\"Partial derivative of Wx with respect to W (row-major)\")\n",
    "print(\"Kronecker Jacobian Iₘ ⊗ xᵀ:\\n\", J, \"\\n\")\n",
    "\n",
    "# Show the structure of the row-major Jacobian\n",
    "print(\"Row-major structure:\")\n",
    "print(\"[[x1  x2  x3   0   0   0 ]\")\n",
    "print(\" [ 0   0   0  x1  x2  x3]]\\n\")\n",
    "\n",
    "for i, p, q in itertools.product(range(m), repeat=3):\n",
    "    j = p*n + q  # For row-major layout: j = p*n + q maps (p,q) to column index\n",
    "                  # where p is output index (0..m-1), q is input index (0..n-1)\n",
    "                  # This formula places each input x[q] in the correct column\n",
    "                  # based on which output row p it contributes to\n",
    "    if q < n:   # keep q in 0..n-1\n",
    "        lhs = J[i, j].item()  # Left-hand side: actual Jacobian value\n",
    "        rhs = (1.0 if i==p else 0.0)*x[q].item()  # Right-hand side: expected value (Kronecker delta * input)\n",
    "        print(f\"i={i}, p={p}, q={q}:  J = {lhs:.2f}  δ·x = {rhs:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
