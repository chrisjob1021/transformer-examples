{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative of Wx with respect to W (col-major)\n",
      "Kronecker Jacobian Iₘ ⊗ xᵀ:\n",
      " tensor([[ 1.3000,  0.0000,  0.9000,  0.0000, -0.7000, -0.0000],\n",
      "        [ 0.0000,  1.3000,  0.0000,  0.9000, -0.0000, -0.7000]]) \n",
      "\n",
      "Weight matrix W:\n",
      " tensor([[ 0.5000, -0.3000,  0.2000],\n",
      "        [ 0.2000,  0.8000, -0.1000]], requires_grad=True) \n",
      "\n",
      "Col-major structure:\n",
      "[[x1  0  x2   0   x3   0 ]\n",
      " [ 0   x1   0  x2  0  x3]]\n",
      "\n",
      "i=0, p=0, q=0:  J = 1.30  δ·x = 1.30\n",
      "i=0, p=0, q=1:  J = 0.90  δ·x = 0.90\n",
      "i=0, p=1, q=0:  J = 0.00  δ·x = 0.00\n",
      "i=0, p=1, q=1:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=0:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=1:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=1, q=0:  J = 1.30  δ·x = 1.30\n",
      "i=1, p=1, q=1:  J = 0.90  δ·x = 0.90\n",
      "\n",
      "Weighted output y = Wx:\n",
      " tensor([0.2400, 1.0500], grad_fn=<MvBackward0>)\n",
      "\n",
      "Manual gradient (xᵀ ⊗ Iₘ):\n",
      " tensor([[ 1.3000,  0.0000,  0.9000,  0.0000, -0.7000, -0.0000],\n",
      "        [ 0.0000,  1.3000,  0.0000,  0.9000, -0.0000, -0.7000]])\n",
      "\n",
      "Original autograd gradient:\n",
      " tensor([[ 1.3000,  0.9000, -0.7000],\n",
      "        [ 1.3000,  0.9000, -0.7000]])\n",
      "\n",
      "Autograd gradient (reshaped):\n",
      " tensor([[ 1.3000,  0.0000,  0.9000,  0.0000, -0.7000,  0.0000],\n",
      "        [ 0.0000,  1.3000,  0.0000,  0.9000,  0.0000, -0.7000]])\n",
      "\n",
      "Gradients match: True\n"
     ]
    }
   ],
   "source": [
    "# row-major (C / PyTorch default)\t\n",
    "#“weights are stored row-by-row, so route the row-gradient (Iₘ) first, then scale each entire row by the inputs (xᵀ).”\n",
    "\n",
    "import torch, itertools\n",
    "m, n = 2, 3  # 2 outputs, 3 inputs\n",
    "x = torch.tensor([1.3, 0.9, -0.7])  # 3D input vector\n",
    "W = torch.tensor([[0.5, -0.3, 0.2], \n",
    "                  [0.2, 0.8, -0.1]], requires_grad=True)  # 2x3 weight matrix\n",
    "I = torch.eye(m)\n",
    "\n",
    "# Col-major Kronecker product: Iₘ ⊗ xᵀ\n",
    "J = torch.kron(x.unsqueeze(0), I)  # (m × m n)\n",
    "\n",
    "print(\"Partial derivative of Wx with respect to W (col-major)\")\n",
    "print(\"Kronecker Jacobian Iₘ ⊗ xᵀ:\\n\", J, \"\\n\")\n",
    "print(\"Weight matrix W:\\n\", W, \"\\n\")\n",
    "\n",
    "# Show the structure of the row-major Jacobian\n",
    "print(\"Col-major structure:\")\n",
    "print(\"[[x1  0  x2   0   x3   0 ]\")\n",
    "print(\" [ 0   x1   0  x2  0  x3]]\\n\")\n",
    "\n",
    "# Row in W corresponds to a neuron\n",
    "# Column in W corresponds to a weight that will multiply with feature in x (column in x)\n",
    "# Result is the neuron's activation\n",
    "# Ji,j = x_t * I_m = δ_{ip} * x_q\n",
    "\n",
    "# Verify the Jacobian entries\n",
    "for i, p, q in itertools.product(range(m), repeat=3):\n",
    "    j = q*m + p\n",
    "    if q < n:   # keep q in 0..n-1\n",
    "        lhs = J[i, j].item()\n",
    "        rhs = (1.0 if i==p else 0.0)*x[q].item()\n",
    "        print(f\"i={i}, p={p}, q={q}:  J = {lhs:.2f}  δ·x = {rhs:.2f}\")\n",
    "\n",
    "# Compute the output and compare manual vs autograd gradients\n",
    "y = W @ x\n",
    "print(\"\\nWeighted output y = Wx:\\n\", y)\n",
    "\n",
    "# Manual gradient computation\n",
    "manual_grad = torch.kron(x.unsqueeze(0), I)  # Iₘ ⊗ xᵀ\n",
    "print(\"\\nManual gradient (xᵀ ⊗ Iₘ):\\n\", manual_grad)\n",
    "\n",
    "# Compute gradients using autograd by backpropagating ones through the network\n",
    "y.backward(torch.ones_like(y))  # Compute gradients\n",
    "autograd_grad = W.grad  # Get raw gradients in shape [2, 3]\n",
    "\n",
    "print(\"\\nOriginal autograd gradient:\\n\", autograd_grad)\n",
    "\n",
    "\n",
    "# Reshape autograd gradients to match manual gradient shape [2, 6]\n",
    "# Initialize a tensor of zeros with shape [m, m*n] to store the reshaped gradients\n",
    "# This creates a 2x6 tensor since m=2 and n=3\n",
    "autograd_grad_reshaped = torch.zeros(m, m*n)  \n",
    "\n",
    "# Iterate through each output neuron (m=2)\n",
    "for i in range(m):\n",
    "    # Iterate through each input feature (n=3)\n",
    "    for j in range(n):\n",
    "        # Place each gradient in column-major order\n",
    "        # j*m + i calculates the correct column index in the reshaped tensor\n",
    "        # This formula maps from the original [m,n] shape to the column-major [m,m*n] shape\n",
    "        # For example, with m=2, n=3:\n",
    "        # - When j=0, i=0: 0*2 + 0 = 0  (first column)\n",
    "        # - When j=0, i=1: 0*2 + 1 = 1  (second column)\n",
    "        # - When j=1, i=0: 1*2 + 0 = 2  (third column)\n",
    "        # - When j=1, i=1: 1*2 + 1 = 3  (fourth column)\n",
    "        # And so on, creating a column-major arrangement of the gradients\n",
    "        # This ensures gradients are arranged in blocks by input feature\n",
    "        autograd_grad_reshaped[i, j*m + i] = autograd_grad[i, j]  \n",
    "\n",
    "print(\"\\nAutograd gradient (reshaped):\\n\", autograd_grad_reshaped)\n",
    "\n",
    "# Verify they match\n",
    "print(\"\\nGradients match:\", torch.allclose(manual_grad, autograd_grad_reshaped))\n",
    "\n",
    "# i is which activation we are taking the derivative of (e.g. a_0)\n",
    "# p is the weight you perturb (i.e. which neuron the weight belongs to) -- row in W\n",
    "# q is the column index of the weight (which input feature it multiplies) -- e.g. x_1\n",
    "\n",
    "# The weight matrix W has rows that correspond one-to-one to the output neurons.\n",
    "# Row 0 are all the weights feeding neuron 0\n",
    "\n",
    "# a_i is the pre-activation (the weighted sum before any non-linearity) of neuron i\n",
    "# The activation vector a collects those neurons: a_0, a_1, ..., a_{m-1}\n",
    "# So a_0 is literally the output of \"neuron 0\": a_0 = W0,q @ x_q\n",
    "\n",
    "# This is why the ∂a_i / ∂W_pq = δ_ip * x_q\n",
    "# If you perturb a weight in same row p, the change in activation i is \"input * weight change\"\n",
    "# If you perturb a weight in different row p, the change in activation i is 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative of Wx with respect to W (row-major)\n",
      "Kronecker Jacobian Iₘ ⊗ xᵀ:\n",
      " tensor([[ 1.3000,  0.9000, -0.7000,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  1.3000,  0.9000, -0.7000]]) \n",
      "\n",
      "Weight matrix W:\n",
      " tensor([[ 0.5000, -0.3000,  0.2000],\n",
      "        [ 0.2000,  0.8000, -0.1000]], requires_grad=True) \n",
      "\n",
      "Row-major structure:\n",
      "[[x1  x2  x3   0   0   0 ]\n",
      " [ 0   0   0  x1  x2  x3]]\n",
      "\n",
      "i=0, p=0, q=0:  J = 1.30  δ·x = 1.30\n",
      "i=0, p=0, q=1:  J = -0.70  δ·x = 0.90\n",
      "i=0, p=1, q=0:  J = 0.90  δ·x = 0.00\n",
      "i=0, p=1, q=1:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=0:  J = 0.00  δ·x = 0.00\n",
      "i=1, p=0, q=1:  J = -0.00  δ·x = 0.00\n",
      "i=1, p=1, q=0:  J = 0.00  δ·x = 1.30\n",
      "i=1, p=1, q=1:  J = 1.30  δ·x = 0.90\n",
      "\n",
      "Weighted output y = Wx:\n",
      " tensor([0.2400, 1.0500], grad_fn=<MvBackward0>)\n",
      "\n",
      "Manual gradient (Iₘ ⊗ xᵀ):\n",
      " tensor([[ 1.3000,  0.9000, -0.7000,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  1.3000,  0.9000, -0.7000]])\n",
      "\n",
      "Autograd gradient:\n",
      " tensor([[ 1.3000,  0.9000, -0.7000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.3000,  0.9000, -0.7000]])\n",
      "\n",
      "Gradients match: True\n"
     ]
    }
   ],
   "source": [
    "# column-major (Fortran / MATLAB default)\n",
    "#“weights are stored column-by-column, so route the column-gradient (xᵀ) first, then scale each entire column by the inputs (Iₘ).\n",
    "\n",
    "import torch, itertools\n",
    "m, n = 2, 3  # 2 outputs, 3 inputs\n",
    "x = torch.tensor([1.3, 0.9, -0.7])  # 3D input vector\n",
    "W = torch.tensor([[0.5, -0.3, 0.2], \n",
    "                  [0.2, 0.8, -0.1]], requires_grad=True)  # 2x3 weight matrix\n",
    "I = torch.eye(m)\n",
    "\n",
    "# Row-major Kronecker product: Iₘ ⊗ xᵀ\n",
    "J = torch.kron(I, x.unsqueeze(0))  # (m × m n)\n",
    "\n",
    "print(\"Partial derivative of Wx with respect to W (row-major)\")\n",
    "print(\"Kronecker Jacobian Iₘ ⊗ xᵀ:\\n\", J, \"\\n\")\n",
    "print(\"Weight matrix W:\\n\", W, \"\\n\")\n",
    "\n",
    "# Show the structure of the row-major Jacobian\n",
    "print(\"Row-major structure:\")\n",
    "print(\"[[x1  x2  x3   0   0   0 ]\")\n",
    "print(\" [ 0   0   0  x1  x2  x3]]\\n\")\n",
    "\n",
    "# Verify the Jacobian entries\n",
    "for i, p, q in itertools.product(range(m), repeat=3):\n",
    "    j = q*m + p\n",
    "    if q < n:   # keep q in 0..n-1\n",
    "        lhs = J[i, j].item()\n",
    "        rhs = (1.0 if i==p else 0.0)*x[q].item()\n",
    "        print(f\"i={i}, p={p}, q={q}:  J = {lhs:.2f}  δ·x = {rhs:.2f}\")\n",
    "\n",
    "# Compute the output and compare manual vs autograd gradients\n",
    "y = W @ x\n",
    "print(\"\\nWeighted output y = Wx:\\n\", y)\n",
    "\n",
    "# Manual gradient computation\n",
    "manual_grad = torch.kron(I, x.unsqueeze(0))  # Iₘ ⊗ xᵀ\n",
    "print(\"\\nManual gradient (Iₘ ⊗ xᵀ):\\n\", manual_grad)\n",
    "\n",
    "# Compute gradients using autograd by backpropagating ones through the network\n",
    "# This line computes gradients by backpropagating a tensor of ones through the network\n",
    "# torch.ones_like(y) creates a tensor of ones with the same shape as y\n",
    "# This is a common pattern in PyTorch when you want to compute gradients with respect to all outputs\n",
    "y.backward(torch.ones_like(y))  # Compute gradients\n",
    "autograd_grad = W.grad  # Get raw gradients in shape [2, 3]\n",
    "\n",
    "# Reshape gradients into Kronecker-like structure [2, 6] where each row contains\n",
    "# the input vector x in the appropriate position, with zeros elsewhere\n",
    "autograd_grad = torch.zeros(m, m*n)  # Initialize [2, 6] tensor\n",
    "for i in range(m):\n",
    "    autograd_grad[i, i*n:(i+1)*n] = x  # Place x in the correct positions\n",
    "\n",
    "print(\"\\nAutograd gradient:\\n\", autograd_grad)\n",
    "\n",
    "# Verify they match\n",
    "print(\"\\nGradients match:\", torch.allclose(manual_grad, autograd_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row-major flatten : [0, 1, 2, 3, 4, 5]\n",
      "column-major via .t() : [0, 3, 1, 4, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.tensor([[0, 1, 2],\n",
    "                  [3, 4, 5]])          # 2 × 3\n",
    "\n",
    "print(\"row-major flatten :\", A.reshape(-1).tolist())\n",
    "print(\"column-major via .t() :\", A.t().contiguous().reshape(-1).tolist())\n",
    "\n",
    "\n",
    "# Numerics Identical; SGD sees the same update values.\n",
    "# Code Frameworks (PyTorch, JAX, TF) store tensors row-major, so if\n",
    "#  you ever call flatten()/reshape(-1) you’re in row-major space and\n",
    "#  should use Iₘ ⊗ xᵀ.\n",
    "# Paper algebra Most math texts use column-major because BLAS does,\n",
    "#  so they write xᵀ ⊗ Iₘ. Just swap factors when translating to code.\n",
    "# Once you reshape back to an m x n matrix, everything collapses to a simple outer product\n",
    "#  aL/aW = [diag(g * aL/av)R]x_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
