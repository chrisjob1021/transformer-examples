{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(self, d_model=256, num_heads=4, num_layers=6, vocab_size=10000, max_len=5000, dropout=0.1):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "class AbsolutePositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the sinusoidal positional encoding described in \"Attention Is All You Need\", Vasani et. al. [2017]\n",
    "\n",
    "    pi()   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = config.max_len\n",
    "        self.d_model = config.d_model\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pos_within_vector = torch.arange(0, self.max_len, dtype=float).unsqueeze(1)\n",
    "        every_other_dim = torch.arange(0, self.d_model, 2, dtype=float)\n",
    "        '''\n",
    "        a = 2i/d_model\n",
    "        -ln(10000^a) = -a ln(10000) \n",
    "        exp(ln(10000^-a)) = 10000^(-a) \n",
    "        '''\n",
    "        div_term = torch.exp((math.log(10000.0)) * -every_other_dim / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos_within_vector * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos_within_vector * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as a buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer encoder block.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_encoding = PositionalEncoding(config)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer model, as in Vaswani et al. (2017).\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_within_vector.shape torch.Size([100, 1])\n",
      "div_term.shape torch.Size([6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 12]),\n",
       " tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  2.1378e-01,  ...,  1.0000e+00,\n",
       "           4.6416e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  4.1768e-01,  ...,  9.9999e-01,\n",
       "           9.2832e-04,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 3.7961e-01, -9.2515e-01,  8.8807e-01,  ...,  9.7824e-01,\n",
       "           4.5008e-02,  9.9899e-01],\n",
       "         [-5.7338e-01, -8.1929e-01,  7.6926e-01,  ...,  9.7779e-01,\n",
       "           4.5472e-02,  9.9897e-01],\n",
       "         [-9.9921e-01,  3.9821e-02,  6.1489e-01,  ...,  9.7734e-01,\n",
       "           4.5936e-02,  9.9894e-01]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Absolute Positional Encoding\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "max_len = 100\n",
    "d_model = 12\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "pos_within_vector = torch.arange(0, max_len, dtype=float).unsqueeze(1)\n",
    "every_other_dim = torch.arange(0, d_model, 2, dtype=float)\n",
    "'''\n",
    "a = 2i/d_model\n",
    "-ln(10000^a) = -a ln(10000) \n",
    "exp(ln(10000^-a)) = 10000^(-a) \n",
    "'''\n",
    "div_term = torch.exp((math.log(10000.0)) * -every_other_dim / d_model)\n",
    "print(\"pos_within_vector.shape\", pos_within_vector.shape)\n",
    "print(\"div_term.shape\", div_term.shape)\n",
    "pe[:, 0::2] = torch.sin(pos_within_vector * div_term)\n",
    "pe[:, 1::2] = torch.cos(pos_within_vector * div_term)\n",
    "pe = pe.unsqueeze(0)\n",
    "pe.shape, pe[0]\n",
    "\n",
    "# pe[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
