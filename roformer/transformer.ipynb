{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from attention import AbsolutePositionalEncoding as PositionalEncoding\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(self, d_model=256, num_heads=4, num_layers=6, vocab_size=10000, max_len=5000, dropout=0.1):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer encoder block.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_encoding = PositionalEncoding(config)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer model, as in Vaswani et al. (2017).\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position.shape torch.Size([100, 1]) tensor([[ 0.],\n",
      "        [ 1.],\n",
      "        [ 2.],\n",
      "        [ 3.],\n",
      "        [ 4.],\n",
      "        [ 5.],\n",
      "        [ 6.],\n",
      "        [ 7.],\n",
      "        [ 8.],\n",
      "        [ 9.],\n",
      "        [10.],\n",
      "        [11.],\n",
      "        [12.],\n",
      "        [13.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [16.],\n",
      "        [17.],\n",
      "        [18.],\n",
      "        [19.],\n",
      "        [20.],\n",
      "        [21.],\n",
      "        [22.],\n",
      "        [23.],\n",
      "        [24.],\n",
      "        [25.],\n",
      "        [26.],\n",
      "        [27.],\n",
      "        [28.],\n",
      "        [29.],\n",
      "        [30.],\n",
      "        [31.],\n",
      "        [32.],\n",
      "        [33.],\n",
      "        [34.],\n",
      "        [35.],\n",
      "        [36.],\n",
      "        [37.],\n",
      "        [38.],\n",
      "        [39.],\n",
      "        [40.],\n",
      "        [41.],\n",
      "        [42.],\n",
      "        [43.],\n",
      "        [44.],\n",
      "        [45.],\n",
      "        [46.],\n",
      "        [47.],\n",
      "        [48.],\n",
      "        [49.],\n",
      "        [50.],\n",
      "        [51.],\n",
      "        [52.],\n",
      "        [53.],\n",
      "        [54.],\n",
      "        [55.],\n",
      "        [56.],\n",
      "        [57.],\n",
      "        [58.],\n",
      "        [59.],\n",
      "        [60.],\n",
      "        [61.],\n",
      "        [62.],\n",
      "        [63.],\n",
      "        [64.],\n",
      "        [65.],\n",
      "        [66.],\n",
      "        [67.],\n",
      "        [68.],\n",
      "        [69.],\n",
      "        [70.],\n",
      "        [71.],\n",
      "        [72.],\n",
      "        [73.],\n",
      "        [74.],\n",
      "        [75.],\n",
      "        [76.],\n",
      "        [77.],\n",
      "        [78.],\n",
      "        [79.],\n",
      "        [80.],\n",
      "        [81.],\n",
      "        [82.],\n",
      "        [83.],\n",
      "        [84.],\n",
      "        [85.],\n",
      "        [86.],\n",
      "        [87.],\n",
      "        [88.],\n",
      "        [89.],\n",
      "        [90.],\n",
      "        [91.],\n",
      "        [92.],\n",
      "        [93.],\n",
      "        [94.],\n",
      "        [95.],\n",
      "        [96.],\n",
      "        [97.],\n",
      "        [98.],\n",
      "        [99.]], dtype=torch.float64)\n",
      "every_other_dim.shape torch.Size([6]) tensor([ 0.,  2.,  4.,  6.,  8., 10.], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 12]),\n",
       " tensor([8.4147e-01, 5.4030e-01, 2.1378e-01, 9.7688e-01, 4.6399e-02, 9.9892e-01,\n",
       "         9.9998e-03, 9.9995e-01, 2.1544e-03, 1.0000e+00, 4.6416e-04, 1.0000e+00]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Absolute Positional Encoding\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "max_len = 100\n",
    "d_model = 12\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=float).unsqueeze(1)\n",
    "print(\"position.shape\", position.shape, position)\n",
    "every_other_dim = torch.arange(0, d_model, 2, dtype=float)\n",
    "print(\"every_other_dim.shape\", every_other_dim.shape, every_other_dim)\n",
    "'''\n",
    "a = 2i/d_model\n",
    "-ln(10000^a) = -a ln(10000) \n",
    "exp(ln(10000^-a)) = 10000^(-a) \n",
    "'''\n",
    "div_term = torch.exp((math.log(10000.0)) * -every_other_dim / d_model)\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)\n",
    "pe.shape, pe[0][1]\n",
    "\n",
    "# pe[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class RelativeMultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    An example implementation of multi-head self-attention with\n",
    "    relative position representations, following Shaw et al. (2018).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, num_heads=4, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Projections for the usual Q, K, V\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj   = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj   = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.relative_key_embeddings = nn.Embedding(2 * max_len + 1, self.head_dim)\n",
    "        self.relative_value_embeddings = nn.Embedding(2 * max_len + 1, self.head_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        q = self.query_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # --- 1. Content-based (standard) attention score ---\n",
    "        # scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "        # --- 2. Incorporate relative position (Key) ---\n",
    "        # Build a matrix of relative position offsets for each pair (i, j)\n",
    "        pos_ids = torch.arange(seq_len).unsqueeze(1) - torch.arange(seq_len)\n",
    "\n",
    "        # If sequence is seq_len, then pos_ids is in range [-(seq_len-1), (seq_len-1)]\n",
    "        # Shifting by max_len - 1 puts the range in [0, 2*(max_len-1)]\n",
    "        # which is the range of the relative key embeddings\n",
    "        # that covers all relative distances [-(max_len-1), max_len-1]\n",
    "        # Further, clamping to 2*(max_len-1) ensures that we do not go out of bounds\n",
    "        pos_ids = pos_ids + (self.max_len - 1)\n",
    "        pos_ids = pos_ids.clamp(0, 2*self.max_len + 1) # shape (seq_len, seq_len)\n",
    "\n",
    "        rel_k = self.relative_key_embeddings(pos_ids) # shape (seq_len, seq_len, head_dim)\n",
    "        scores_r = torch.matmul(q, rel_k.transpose(-2, -1))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_ids.shape torch.Size([12, 12]) tensor([[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n",
      "        [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],\n",
      "        [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],\n",
      "        [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],\n",
      "        [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],\n",
      "        [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5],\n",
      "        [17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6],\n",
      "        [18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7],\n",
      "        [19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8],\n",
      "        [20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9],\n",
      "        [21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10],\n",
      "        [22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 12\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # (B, L, d_model)\n",
    "\n",
    "# Create the module\n",
    "attn = RelativeMultiHeadSelfAttention(d_model, num_heads, max_len=seq_len)\n",
    "\n",
    "# Forward pass\n",
    "output = attn(x)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
