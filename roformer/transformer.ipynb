{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from attention import AbsolutePositionalEncoding as PositionalEncoding\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(self, d_model=256, num_heads=4, num_layers=6, vocab_size=10000, max_len=5000, dropout=0.1):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer encoder block.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_encoding = PositionalEncoding(config)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer model, as in Vaswani et al. (2017).\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position.shape torch.Size([100, 1]) tensor([[ 0.],\n",
      "        [ 1.],\n",
      "        [ 2.],\n",
      "        [ 3.],\n",
      "        [ 4.],\n",
      "        [ 5.],\n",
      "        [ 6.],\n",
      "        [ 7.],\n",
      "        [ 8.],\n",
      "        [ 9.],\n",
      "        [10.],\n",
      "        [11.],\n",
      "        [12.],\n",
      "        [13.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [16.],\n",
      "        [17.],\n",
      "        [18.],\n",
      "        [19.],\n",
      "        [20.],\n",
      "        [21.],\n",
      "        [22.],\n",
      "        [23.],\n",
      "        [24.],\n",
      "        [25.],\n",
      "        [26.],\n",
      "        [27.],\n",
      "        [28.],\n",
      "        [29.],\n",
      "        [30.],\n",
      "        [31.],\n",
      "        [32.],\n",
      "        [33.],\n",
      "        [34.],\n",
      "        [35.],\n",
      "        [36.],\n",
      "        [37.],\n",
      "        [38.],\n",
      "        [39.],\n",
      "        [40.],\n",
      "        [41.],\n",
      "        [42.],\n",
      "        [43.],\n",
      "        [44.],\n",
      "        [45.],\n",
      "        [46.],\n",
      "        [47.],\n",
      "        [48.],\n",
      "        [49.],\n",
      "        [50.],\n",
      "        [51.],\n",
      "        [52.],\n",
      "        [53.],\n",
      "        [54.],\n",
      "        [55.],\n",
      "        [56.],\n",
      "        [57.],\n",
      "        [58.],\n",
      "        [59.],\n",
      "        [60.],\n",
      "        [61.],\n",
      "        [62.],\n",
      "        [63.],\n",
      "        [64.],\n",
      "        [65.],\n",
      "        [66.],\n",
      "        [67.],\n",
      "        [68.],\n",
      "        [69.],\n",
      "        [70.],\n",
      "        [71.],\n",
      "        [72.],\n",
      "        [73.],\n",
      "        [74.],\n",
      "        [75.],\n",
      "        [76.],\n",
      "        [77.],\n",
      "        [78.],\n",
      "        [79.],\n",
      "        [80.],\n",
      "        [81.],\n",
      "        [82.],\n",
      "        [83.],\n",
      "        [84.],\n",
      "        [85.],\n",
      "        [86.],\n",
      "        [87.],\n",
      "        [88.],\n",
      "        [89.],\n",
      "        [90.],\n",
      "        [91.],\n",
      "        [92.],\n",
      "        [93.],\n",
      "        [94.],\n",
      "        [95.],\n",
      "        [96.],\n",
      "        [97.],\n",
      "        [98.],\n",
      "        [99.]], dtype=torch.float64)\n",
      "every_other_dim.shape torch.Size([6]) tensor([ 0.,  2.,  4.,  6.,  8., 10.], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 12]),\n",
       " tensor([8.4147e-01, 5.4030e-01, 2.1378e-01, 9.7688e-01, 4.6399e-02, 9.9892e-01,\n",
       "         9.9998e-03, 9.9995e-01, 2.1544e-03, 1.0000e+00, 4.6416e-04, 1.0000e+00]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Absolute Positional Encoding\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "max_len = 100\n",
    "d_model = 12\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=float).unsqueeze(1)\n",
    "print(\"position.shape\", position.shape, position)\n",
    "every_other_dim = torch.arange(0, d_model, 2, dtype=float)\n",
    "print(\"every_other_dim.shape\", every_other_dim.shape, every_other_dim)\n",
    "'''\n",
    "a = 2i/d_model\n",
    "-ln(10000^a) = -a ln(10000) \n",
    "exp(ln(10000^-a)) = 10000^(-a) \n",
    "'''\n",
    "div_term = torch.exp((math.log(10000.0)) * -every_other_dim / d_model)\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)\n",
    "pe.shape, pe[0][1]\n",
    "\n",
    "# pe[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class RelativeMultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    An example implementation of multi-head self-attention with\n",
    "    relative position representations, following Shaw et al. (2018).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, num_heads=4, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Projections for the usual Q, K, V\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj   = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj   = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.relative_key_embeddings = nn.Embedding(2 * max_len + 1, self.head_dim)\n",
    "        self.relative_value_embeddings = nn.Embedding(2 * max_len + 1, self.head_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        q = self.query_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # --- Content-based (standard) attention score ---\n",
    "        # qk shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "        # --- Incorporate relative position (Key) ---\n",
    "        # Build a matrix of relative position offsets for each pair (i, j)\n",
    "        pos_ids = torch.arange(seq_len).unsqueeze(1) - torch.arange(seq_len)\n",
    "\n",
    "        # If sequence is seq_len, then pos_ids is in range [-(seq_len), (seq_len)]\n",
    "        # Shifting by max_len puts the range in [0, 2 * max_len + 1]\n",
    "        # which is the range of the relative key embeddings\n",
    "        pos_ids = pos_ids + (self.max_len)\n",
    "        pos_ids = pos_ids.clamp(0, 2*self.max_len + 1)                      # shape (seq_len, seq_len)\n",
    "\n",
    "        rel_k = self.relative_key_embeddings(pos_ids)                       # shape (seq_len, seq_len, head_dim)\n",
    "        q_r = q.unsqueeze(3)                                                # shape (batch_size, num_heads, seq_len, 1, head_dim)\n",
    "        rel_k = rel_k.unsqueeze(0).unsqueeze(0)                             # shape (1, 1, seq_len, seq_len, head_dim)\n",
    "        qk_r = torch.matmul(q_r, rel_k.transpose(-2, -1)).squeeze(-2)       # shape (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        qk = qk + qk_r\n",
    "        qk_r = qk / math.sqrt(self.head_dim)\n",
    "        probs = torch.softmax(qk, dim=-1)                                  # shape (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        attn = torch.matmul(probs, v) # scores torch.Size([2, 4, 12, 4])          \n",
    "\n",
    "        # attn = torch.matmul(scores, v) # scores torch.Size([2, 4, 12, 4])          \n",
    "        # print(\"attn\", attn.shape)                          \n",
    "\n",
    "        rel_v = self.relative_value_embeddings(pos_ids)                     # shape (seq_len, seq_len, head_dim)\n",
    "        print(\"rel_v\", rel_v.shape)\n",
    "\n",
    "        # Typically we can do a matrix multiplication of the attention probabilities and the values\n",
    "        # For relative embeddings, we have a separate embedding for each possible distance, per head\n",
    "        # So, instead of a single matrix with size (seq_len, head_dim), we have a matrix with size (seq_len, seq_len, head_dim)\n",
    "        # Î±(i,j) * aV(i,j)\n",
    "        # Step 1 - element-wise multiply each attention weight by the corresponding relative embedding\n",
    "        probs_r = probs.unsqueeze(-1)                                       # shape (batch_size, num_heads, seq_len, seq_len, 1)\n",
    "        rel_v = rel_v.unsqueeze(0).unsqueeze(0)                             # shape (1, 1, seq_len, seq_len, dim_head)\n",
    "        attn_r = probs_r * rel_v                                            # shape (batch_size, num_heads, seq_len, seq_len, dim_head)\n",
    "        # Step 2 - sum over the dimension j\n",
    "        attn_r = attn_r.sum(dim=3)                                           # shape (batch_size, num_heads, seq_len, dim_head)\n",
    "        \n",
    "        attn = attn + attn_r\n",
    "        # Reshape to the original dimensions and consolidate all of the heads\n",
    "        out = attn.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        # And linearly project\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel_v torch.Size([12, 12, 4])\n",
      "output torch.Size([2, 12, 16]) tensor([[[ 7.0756e-02,  5.5792e-01, -3.5239e-01, -3.8813e-02,  3.1118e-02,\n",
      "          -7.2510e-02, -2.5472e-01,  2.1883e-01,  4.3866e-01, -6.3810e-01,\n",
      "          -4.8520e-01,  2.7380e-01,  1.7937e-02, -1.3297e-01, -2.8839e-01,\n",
      "          -2.7720e-01],\n",
      "         [ 2.2258e-01, -2.6789e-01,  1.1867e-01,  2.2818e-01, -6.2169e-01,\n",
      "          -1.3462e-01,  1.5293e-01,  9.9110e-02,  1.6223e-01, -2.0867e-01,\n",
      "           2.0280e-01,  8.6187e-01,  6.1669e-02,  2.2743e-01,  3.1709e-01,\n",
      "           2.9780e-01],\n",
      "         [-7.3058e-01, -5.7413e-01, -6.7851e-01,  3.4520e-02,  3.1706e-01,\n",
      "           3.0778e-01,  3.0434e-01, -5.7832e-01,  1.0315e+00,  2.6030e-01,\n",
      "          -7.0750e-02,  6.3350e-01, -2.5737e-01, -5.5300e-01,  5.7763e-01,\n",
      "           1.5898e-01],\n",
      "         [ 9.9492e-02, -1.2708e-01,  3.8734e-01,  6.8186e-01, -8.5880e-01,\n",
      "          -8.3040e-01, -4.8952e-01, -1.4007e-03,  3.2401e-01, -5.1098e-01,\n",
      "          -3.6689e-01,  6.2249e-01,  7.2152e-01,  5.0472e-02, -5.8836e-01,\n",
      "          -5.4150e-01],\n",
      "         [-7.2209e-01,  2.8446e-01, -3.7957e-01,  5.8739e-02,  4.1459e-01,\n",
      "           1.8943e-02, -2.7994e-01, -3.0259e-01,  2.4104e-01, -2.5705e-01,\n",
      "          -2.0917e-01,  2.4253e-01, -1.1087e-01, -9.5271e-02, -4.8560e-01,\n",
      "          -2.8707e-01],\n",
      "         [-4.9877e-01, -6.3553e-01, -9.2547e-02,  1.8829e-02,  9.3676e-02,\n",
      "          -2.5681e-03,  1.5785e-01, -5.8018e-01,  1.6870e-01, -4.8961e-02,\n",
      "           2.1462e-01,  4.6402e-01,  1.4574e-01,  1.1269e-01, -4.2435e-01,\n",
      "          -1.6839e-01],\n",
      "         [-3.4125e-01, -4.6840e-01,  7.0373e-01,  4.2919e-01, -1.1097e-02,\n",
      "          -4.0908e-01,  2.2555e-01, -1.0040e+00, -9.9009e-01,  6.8320e-03,\n",
      "          -1.6750e-01,  4.6760e-01,  9.3587e-01,  1.1322e-02, -7.6855e-01,\n",
      "          -6.5125e-01],\n",
      "         [-6.8083e-01,  6.9717e-02, -7.4696e-01, -2.6912e-02,  5.8557e-01,\n",
      "           3.5953e-01,  1.2061e-01, -7.9003e-01,  9.7161e-02,  2.0313e-01,\n",
      "           6.4345e-02, -3.6420e-02, -2.8318e-01, -2.2418e-01, -2.7071e-01,\n",
      "          -2.6106e-01],\n",
      "         [-2.5393e-01,  1.2946e-01, -7.9705e-01, -1.4647e-01,  2.4767e-01,\n",
      "           2.7118e-01,  1.3161e-01, -2.1843e-01,  5.5509e-01, -1.2285e-01,\n",
      "           2.8529e-01,  2.3915e-01, -3.6988e-01,  3.2405e-01, -7.2446e-02,\n",
      "           2.9731e-01],\n",
      "         [-4.4630e-01, -4.4154e-01, -2.5964e-01,  2.1584e-01,  7.7005e-02,\n",
      "           1.2409e-01,  9.2016e-02, -3.5233e-01,  5.9421e-01, -2.5995e-01,\n",
      "          -9.0278e-02,  5.2677e-01,  1.5060e-01,  3.3450e-01, -1.0874e-01,\n",
      "           1.1195e-01],\n",
      "         [ 1.0046e-02,  6.7178e-02, -1.0046e+00, -2.0510e-01, -7.5354e-02,\n",
      "           3.6075e-02,  1.5096e-01, -3.5664e-01,  6.8958e-01, -6.4916e-01,\n",
      "           1.8864e-01,  5.0686e-01, -4.0325e-01, -8.3668e-02, -1.0324e-01,\n",
      "           5.7873e-02],\n",
      "         [-4.4965e-01,  5.6238e-01, -1.0552e+00, -2.1453e-01,  4.4901e-01,\n",
      "           4.0260e-01, -1.7856e-01, -1.2736e-01,  6.6686e-01, -4.0413e-01,\n",
      "          -4.2113e-03,  4.0584e-02, -4.8525e-01, -2.2501e-02, -9.5720e-02,\n",
      "          -1.4889e-01]],\n",
      "\n",
      "        [[-5.5790e-01,  1.5608e-02, -1.9334e-01, -2.0888e-01,  2.6361e-01,\n",
      "           1.1276e-01, -2.2930e-01, -2.2849e-01,  1.4913e-01,  2.4549e-01,\n",
      "           1.4736e-01, -1.8339e-01, -2.8989e-01, -4.2286e-02, -4.8165e-01,\n",
      "          -3.7507e-01],\n",
      "         [ 2.8242e-02, -6.7359e-01,  1.0437e-01, -4.5644e-02, -2.8718e-01,\n",
      "          -2.4465e-01,  3.5986e-01, -3.9039e-01,  1.8828e-01,  1.1661e-01,\n",
      "           3.0983e-01,  3.6961e-01, -7.2396e-02, -5.7726e-02,  4.3323e-02,\n",
      "           2.1132e-01],\n",
      "         [-7.9713e-01,  1.5056e-01, -9.1116e-01,  1.3157e-02,  6.3025e-01,\n",
      "           5.0143e-01,  2.8849e-02, -4.6310e-01,  6.2123e-01,  4.2373e-01,\n",
      "           3.5433e-01, -6.7248e-01, -2.5010e-01, -1.0531e-01, -5.0731e-01,\n",
      "          -2.5383e-01],\n",
      "         [ 1.2301e-01,  1.3913e-01, -9.5298e-02, -2.6706e-01, -2.9763e-03,\n",
      "          -2.3759e-01,  5.7059e-02, -6.8781e-02,  1.0077e-01, -2.4133e-01,\n",
      "           2.0435e-01,  1.5689e-01, -1.1215e-01,  2.1135e-02, -4.2344e-01,\n",
      "          -1.4558e-01],\n",
      "         [-3.7341e-01, -9.2939e-01,  1.6439e-01, -4.8459e-02, -1.1874e-02,\n",
      "           1.6945e-01,  2.1596e-01, -5.0250e-01, -2.1354e-02,  2.8435e-01,\n",
      "           2.8981e-01, -3.0783e-02, -1.9054e-01,  4.0941e-01, -6.5424e-01,\n",
      "           8.3920e-02],\n",
      "         [-1.5166e-01, -4.0798e-01,  1.4834e-01, -1.2075e-02, -1.8182e-01,\n",
      "           2.2764e-01,  1.7376e-01, -2.6118e-01, -9.9212e-02,  1.6823e-01,\n",
      "           2.1457e-01,  3.9764e-03,  2.1227e-01,  5.5015e-01, -6.1202e-01,\n",
      "          -6.4975e-02],\n",
      "         [-5.0307e-01, -3.2813e-01, -2.8037e-01, -3.4819e-01,  4.2160e-01,\n",
      "           7.8237e-02, -3.2671e-01, -4.5061e-01, -2.0967e-01,  8.1980e-02,\n",
      "           2.7296e-01, -4.7166e-01, -1.0681e-01,  2.6511e-01, -1.2343e+00,\n",
      "          -6.6731e-01],\n",
      "         [-6.0968e-01,  9.7780e-02, -8.2498e-01, -6.2157e-01,  7.1081e-01,\n",
      "           2.3012e-01, -1.3898e-01, -6.6237e-01,  4.2833e-02,  2.4900e-01,\n",
      "           5.2446e-01, -4.5452e-01, -4.0981e-01, -6.2993e-03, -7.9277e-01,\n",
      "          -6.4378e-01],\n",
      "         [-3.0884e-01, -4.7761e-01, -7.1925e-01, -1.1222e+00,  6.5178e-01,\n",
      "           4.6217e-01,  8.3403e-02, -7.3197e-01,  8.9462e-02,  1.8516e-01,\n",
      "           3.6591e-01, -9.1662e-01, -6.9614e-01,  5.2165e-01, -1.4122e+00,\n",
      "          -3.0801e-01],\n",
      "         [-2.1583e-01,  2.2167e-01, -5.3769e-01, -1.0152e-01, -2.9237e-03,\n",
      "           2.6340e-01, -1.9900e-01, -1.0914e-01,  1.4943e-01, -1.2699e-01,\n",
      "           1.4643e-01, -3.6858e-01, -2.6047e-01,  2.9722e-02, -5.7220e-01,\n",
      "          -3.4974e-01],\n",
      "         [-7.6259e-01,  2.9731e-01, -5.3600e-01, -5.0023e-01,  6.3857e-01,\n",
      "           2.2752e-01, -3.5113e-01, -5.1378e-01,  4.5229e-02,  2.6522e-01,\n",
      "           1.5245e-01, -4.8469e-01, -3.4798e-01,  3.3747e-03, -9.4749e-01,\n",
      "          -7.3930e-01],\n",
      "         [-7.3985e-01, -1.1786e-01, -2.9085e-01, -6.1262e-01,  6.8756e-01,\n",
      "           4.6377e-01, -5.6068e-02, -4.4779e-01,  1.2221e-01,  5.0137e-01,\n",
      "           2.2070e-01, -7.7918e-01, -4.8475e-01,  2.0515e-01, -1.0325e+00,\n",
      "          -3.7546e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 12\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # (B, L, d_model)\n",
    "\n",
    "# Create the module\n",
    "attn = RelativeMultiHeadSelfAttention(d_model, num_heads, max_len=seq_len)\n",
    "\n",
    "# Forward pass\n",
    "output = attn(x)\n",
    "print(\"output\", output.shape, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
