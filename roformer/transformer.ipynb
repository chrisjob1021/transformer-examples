{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from attention import AbsolutePositionalEncoding as PositionalEncoding\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(self, d_model=256, num_heads=4, num_layers=6, vocab_size=10000, max_len=5000, dropout=0.1):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout = dropout\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer encoder block.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_encoding = PositionalEncoding(config)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer model, as in Vaswani et al. (2017).\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position.shape torch.Size([100, 1]) tensor([[ 0.],\n",
      "        [ 1.],\n",
      "        [ 2.],\n",
      "        [ 3.],\n",
      "        [ 4.],\n",
      "        [ 5.],\n",
      "        [ 6.],\n",
      "        [ 7.],\n",
      "        [ 8.],\n",
      "        [ 9.],\n",
      "        [10.],\n",
      "        [11.],\n",
      "        [12.],\n",
      "        [13.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [16.],\n",
      "        [17.],\n",
      "        [18.],\n",
      "        [19.],\n",
      "        [20.],\n",
      "        [21.],\n",
      "        [22.],\n",
      "        [23.],\n",
      "        [24.],\n",
      "        [25.],\n",
      "        [26.],\n",
      "        [27.],\n",
      "        [28.],\n",
      "        [29.],\n",
      "        [30.],\n",
      "        [31.],\n",
      "        [32.],\n",
      "        [33.],\n",
      "        [34.],\n",
      "        [35.],\n",
      "        [36.],\n",
      "        [37.],\n",
      "        [38.],\n",
      "        [39.],\n",
      "        [40.],\n",
      "        [41.],\n",
      "        [42.],\n",
      "        [43.],\n",
      "        [44.],\n",
      "        [45.],\n",
      "        [46.],\n",
      "        [47.],\n",
      "        [48.],\n",
      "        [49.],\n",
      "        [50.],\n",
      "        [51.],\n",
      "        [52.],\n",
      "        [53.],\n",
      "        [54.],\n",
      "        [55.],\n",
      "        [56.],\n",
      "        [57.],\n",
      "        [58.],\n",
      "        [59.],\n",
      "        [60.],\n",
      "        [61.],\n",
      "        [62.],\n",
      "        [63.],\n",
      "        [64.],\n",
      "        [65.],\n",
      "        [66.],\n",
      "        [67.],\n",
      "        [68.],\n",
      "        [69.],\n",
      "        [70.],\n",
      "        [71.],\n",
      "        [72.],\n",
      "        [73.],\n",
      "        [74.],\n",
      "        [75.],\n",
      "        [76.],\n",
      "        [77.],\n",
      "        [78.],\n",
      "        [79.],\n",
      "        [80.],\n",
      "        [81.],\n",
      "        [82.],\n",
      "        [83.],\n",
      "        [84.],\n",
      "        [85.],\n",
      "        [86.],\n",
      "        [87.],\n",
      "        [88.],\n",
      "        [89.],\n",
      "        [90.],\n",
      "        [91.],\n",
      "        [92.],\n",
      "        [93.],\n",
      "        [94.],\n",
      "        [95.],\n",
      "        [96.],\n",
      "        [97.],\n",
      "        [98.],\n",
      "        [99.]], dtype=torch.float64)\n",
      "every_other_dim.shape torch.Size([6]) tensor([ 0.,  2.,  4.,  6.,  8., 10.], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 12]),\n",
       " tensor([8.4147e-01, 5.4030e-01, 2.1378e-01, 9.7688e-01, 4.6399e-02, 9.9892e-01,\n",
       "         9.9998e-03, 9.9995e-01, 2.1544e-03, 1.0000e+00, 4.6416e-04, 1.0000e+00]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Absolute Positional Encoding\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "max_len = 100\n",
    "d_model = 12\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=float).unsqueeze(1)\n",
    "print(\"position.shape\", position.shape, position)\n",
    "every_other_dim = torch.arange(0, d_model, 2, dtype=float)\n",
    "print(\"every_other_dim.shape\", every_other_dim.shape, every_other_dim)\n",
    "'''\n",
    "a = 2i/d_model\n",
    "-ln(10000^a) = -a ln(10000) \n",
    "exp(ln(10000^-a)) = 10000^(-a) \n",
    "'''\n",
    "div_term = torch.exp((math.log(10000.0)) * -every_other_dim / d_model)\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)\n",
    "pe.shape, pe[0][1]\n",
    "\n",
    "# pe[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class RelativeMultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    An example implementation of multi-head self-attention with\n",
    "    relative position representations, following Shaw et al. (2018).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, num_heads=4, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Projections for the usual Q, K, V\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj   = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj   = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.relative_key_embeddings = nn.Embedding(2 * max_len + 1, self.head_dim)\n",
    "        self.relative_value_embeddings = nn.Embedding(2 * max_len + 1, self.head_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        q = self.query_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # --- Content-based (standard) attention score ---\n",
    "        # qk shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "\n",
    "        # --- Incorporate relative position (Key) ---\n",
    "        # Build a matrix of relative position offsets for each pair (i, j)\n",
    "        pos_ids = torch.arange(seq_len).unsqueeze(1) - torch.arange(seq_len)\n",
    "\n",
    "        # If sequence is seq_len, then pos_ids is in range [-(seq_len), (seq_len)]\n",
    "        # Shifting by max_len puts the range in [0, 2 * max_len + 1]\n",
    "        # which is the range of the relative key embeddings\n",
    "        pos_ids = pos_ids + (self.max_len)\n",
    "        pos_ids = pos_ids.clamp(0, 2*self.max_len + 1)                      # shape (seq_len, seq_len)\n",
    "\n",
    "        rel_k = self.relative_key_embeddings(pos_ids)                       # shape (seq_len, seq_len, head_dim)\n",
    "        q_r = q.unsqueeze(3)                                                # shape (batch_size, num_heads, seq_len, 1, head_dim)\n",
    "        rel_k = rel_k.unsqueeze(0).unsqueeze(0)                             # shape (1, 1, seq_len, seq_len, head_dim)\n",
    "        qk_r = torch.matmul(q_r, rel_k.transpose(-2, -1)).squeeze(-2)       # shape (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        qk = qk + qk_r\n",
    "        qk_r = qk / math.sqrt(self.head_dim)\n",
    "        scores = torch.softmax(qk, dim=-1)                                      # shape (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        attn = torch.matmul(qk, v) # scores torch.Size([2, 4, 12, 4])          \n",
    "        print(\"attn\", attn.shape)                          \n",
    "\n",
    "        rel_v = self.relative_value_embeddings(pos_ids)                     # shape (seq_len, seq_len, head_dim)\n",
    "        scores_r = scores.unsqueeze(-1)\n",
    "        rel_v = rel_v.unsqueeze(0).unsqueeze(0)\n",
    "        print(\"rel_v\", rel_v.shape)\n",
    "\n",
    "        attn_r = torch.matmul(scores_r, rel_v)\n",
    "        print(\"attn_r\", attn_r.shape)\n",
    "        # print(\"v_r.shape\", v_r.shape)\n",
    "\n",
    "        # print(\"v.shape, v_r.shape\", v.shape, v_r.shape)\n",
    "\n",
    "        # print(\"out_r.shape\", out_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn torch.Size([2, 4, 12, 4])\n",
      "rel_v torch.Size([12, 12, 4])\n",
      "rel_v torch.Size([1, 1, 12, 12, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [96, 1] but got: [96, 12].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m attn = RelativeMultiHeadSelfAttention(d_model, num_heads, max_len=seq_len)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m output = \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/transformer-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Development/transformer-examples/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mRelativeMultiHeadSelfAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     64\u001b[39m rel_v = rel_v.unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mrel_v\u001b[39m\u001b[33m\"\u001b[39m, rel_v.shape)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m attn_r = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattn_r\u001b[39m\u001b[33m\"\u001b[39m, attn_r.shape)\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected size for first two dimensions of batch2 tensor to be: [96, 1] but got: [96, 12]."
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 12\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # (B, L, d_model)\n",
    "\n",
    "# Create the module\n",
    "attn = RelativeMultiHeadSelfAttention(d_model, num_heads, max_len=seq_len)\n",
    "\n",
    "# Forward pass\n",
    "output = attn(x)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
